{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# data preparation imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import normaltest\n",
    "from collections import Counter\n",
    "import re\n",
    "import sklearn as sk\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: cleanup imports\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D, GlobalMaxPool1D\n",
    "from keras.layers import merge, Lambda, concatenate, BatchNormalization, Reshape\n",
    "from keras.layers import TimeDistributed, \n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import keras.callbacks\n",
    "\n",
    "import os\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import x for training, the sentences\n",
    "with open('./data/xtrain.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process data to remove empty line\n",
    "xdata = text.split('\\n')\n",
    "xdata = filter(None, xdata)\n",
    "xdata = np.asarray(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import y for training, the ground truth labels\n",
    "with open('./data/ytrain.txt') as f:\n",
    "    labeltext = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess y to remove emtpy string and convert to integers for checks\n",
    "ydata_str = labeltext.split('\\n')\n",
    "ydata_str = filter(None, ydata_str) \n",
    "\n",
    "ydata = [int(numeric_string) for numeric_string in ydata_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the number of sentences and labels is equal: True\n"
     ]
    }
   ],
   "source": [
    "# verify that both x and y have the same number of entries\n",
    "print \"Checking if the number of sentences and labels is equal:\", len(xdata)==len(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually load the list of book titles for nicer prints\n",
    "# I fixed a minor spelling error in one of the book titles here\n",
    "book_titles = ['alice_in_wonderland','dracula','dubliners','great_expectations','hard_times','huckleberry_finn',\n",
    "               'les_miserable','moby_dick','oliver_twist','peter_pan','tale_of_two_cities','tom_sawyer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sanity check for labels\n",
    "# let's take a look at the labels first\n",
    "# it makes sense to take a look at the labels \n",
    "# I like histograms since it's important to check how our labels are distributed \n",
    "bins = range(13)\n",
    "hist, bin_edges = np.histogram(ydata, bins, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2bb42e4310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shows a nice plot of the histogram of labels\n",
    "plt.title(\"Label Histogram\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency in percentage\")\n",
    "plt.bar(bin_edges[:-1], hist, width=1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see in this plot that there is actually not an even distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book alice_in_wonderland, with index 0, has the lowest number of samples, 1.704754 percent.\n",
      "The book moby_dick, with index 7, has the highest number of samples, 15.614689 percent.\n",
      "That means that we have about 9 times more samples from moby_dick than from alice_in_wonderland.\n"
     ]
    }
   ],
   "source": [
    "# print a few interesting numbers about the histogram\n",
    "print (\"The book %s, with index %i, has the lowest number of samples, %f percent.\"\n",
    "%(book_titles[np.argmin(hist)],np.argmin(hist),min(hist)*100) )\n",
    "print (\"The book %s, with index %i, has the highest number of samples, %f percent.\"\n",
    "%(book_titles[np.argmax(hist)],np.argmax(hist),max(hist)*100) )\n",
    "\n",
    "print (\"That means that we have about %i times more samples from %s than from %s.\"\n",
    "%(max(hist)/min(hist),book_titles[np.argmax(hist)],book_titles[np.argmin(hist)]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 58.659498207885306, 1: 9.446464646464646, 2: 21.923643670462155, 3: 8.0323926380368107, 4: 13.88125530110263, 5: 14.132987910189982, 6: 7.741721854304636, 7: 6.4042261788299744, 8: 8.9775095995611629, 9: 32.634097706879359, 10: 10.644552845528455, 11: 23.313390313390311}\n"
     ]
    }
   ],
   "source": [
    "# I've done a few training test runs and realized that the number of labels is screwed so badly that the prediction\n",
    "# didn't work well. For example I obtained this histogram of predicted labels for the test data:\n",
    "# [   0  252    0   72    0    0 2063  434  179    0    0    0] \n",
    "# so some classes were not predicted at all, while only the most common 5 classes were predicted\n",
    "\n",
    "# labels_dict : {ind_label: count_label}\n",
    "\n",
    "def create_class_weight(labels_dict):\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        class_weight[key] = 1.0/labels_dict[key]\n",
    "\n",
    "    return class_weight\n",
    "\n",
    "# random labels_dict\n",
    "\n",
    "keys = range(0,len(book_titles))\n",
    "labels_dict = dict((keys[i],hist[i]) for i in range(len(book_titles)))\n",
    "class_weights = create_class_weight(labels_dict)\n",
    "print class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Clean up sentences\n",
    "# Some early experiments showed that there were some non-ascii characters in the sentence data\n",
    "# I've decided to remove these\n",
    "uni_x = [sen.encode('ascii', 'ignore') for sen in xdata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence in the training data has 189 symbols, while the shortest has 54 symbols.\n",
      "The average sentence length is 167.874404 and the standard deviation is 8.061963.\n"
     ]
    }
   ],
   "source": [
    "## Sanity check for sentences x\n",
    "x_len = [len(sen) for sen in uni_x]\n",
    "min_len = min(x_len)\n",
    "max_len = max(x_len)\n",
    "print \"The longest sentence in the training data has %i symbols, while the shortest has %i symbols.\"%(max_len,min_len)\n",
    "print(\"The average sentence length is %f and the standard deviation is %f.\"% (np.mean(x_len),np.std(x_len)))\n",
    "\n",
    "# so the length of the sentences varies but not that much, especially the very short sentences seem to be outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    0    0    0    0    1    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    1    2    1    0    0\n",
      "    0    0    0    0    0    0    1    0    0    0    1    0    1    0    0\n",
      "    0    0    0    0    0    0    0    1    0    1    1    0    0    0    0\n",
      "    2    5    0    1    4    3    3    1    2    4    2    4    5    6    6\n",
      "   11   10    6   13   19   18   15   33   23   18   38   43   40   49   64\n",
      "   58   76   90   92  143  144  158  194  196  260  280  313  346  410  430\n",
      "  480  577  645  791  914 1083 1259 1476 1760 1893 2122 2245 2245 2212 2029\n",
      " 1783 1584 1239  904  688  469  286  196   80   59   29   19    8    6]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNcSJw8zMGuLEYWZmDXHiMDOzhvx/4lQmCPupI8AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2bb42dee10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = range(min_len,max_len,1)\n",
    "hist, bin_edges = np.histogram(x_len,bins,density=False )\n",
    "print hist\n",
    "\n",
    "# shows a nice plot of the histogram of sentence lengths\n",
    "plt.title(\"Sentence length Histogram\")\n",
    "plt.xlabel(\"Sequence length\")\n",
    "plt.ylabel(\"Frequency absolute numbers\")\n",
    "plt.bar(bin_edges[:-1], hist, width = 1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see in this plot that the sentence length appears to be normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistical test for: p value for normality = 0.000000 .\n"
     ]
    }
   ],
   "source": [
    "print(\"statistical test for: p value for normality = %f .\"%(normaltest(x_len)[1]) )\n",
    "\n",
    "# theory that the data was picked with normally distributed sentence length can't be rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histogram_letters(L):\n",
    "    d = Counter(letter for line in L for letter in line)\n",
    "    for letter in d:\n",
    "        print('{} | {}'.format(letter, d[letter]))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! | 6720\n",
      "# | 3\n",
      "\" | 4520\n",
      "$ | 3\n",
      "' | 10332\n",
      "& | 15\n",
      ") | 804\n",
      "( | 833\n",
      "* | 48\n",
      "- | 19931\n",
      ", | 108481\n",
      "/ | 3\n",
      ". | 50053\n",
      "1 | 364\n",
      "0 | 223\n",
      "3 | 113\n",
      "2 | 159\n",
      "5 | 123\n",
      "4 | 73\n",
      "7 | 96\n",
      "6 | 48\n",
      "9 | 64\n",
      "8 | 159\n",
      "; | 15494\n",
      ": | 2440\n",
      "= | 11\n",
      "< | 6\n",
      "? | 4670\n",
      "> | 6\n",
      "[ | 116\n",
      "] | 103\n",
      "_ | 3365\n",
      "a | 428514\n",
      "c | 118709\n",
      "b | 83080\n",
      "e | 651182\n",
      "d | 242642\n",
      "g | 112092\n",
      "f | 112291\n",
      "i | 357560\n",
      "h | 353582\n",
      "k | 46716\n",
      "j | 8225\n",
      "m | 135069\n",
      "l | 211696\n",
      "o | 402570\n",
      "n | 364930\n",
      "q | 5371\n",
      "p | 85532\n",
      "s | 327557\n",
      "r | 293094\n",
      "u | 147819\n",
      "t | 484089\n",
      "w | 134334\n",
      "v | 46785\n",
      "y | 103313\n",
      "x | 6212\n",
      "{ | 4\n",
      "z | 2515\n",
      "} | 3\n"
     ]
    }
   ],
   "source": [
    "## histogram of the letters\n",
    "l_hist = histogram_letters(uni_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For embedding the sentences, we will use an alphabet with 46 letters with at least 100 occurences in the training set.\n"
     ]
    }
   ],
   "source": [
    "## analysis\n",
    "# 'e', 't' and 'a' are most common letters, which was expectable for a dataset using English language text\n",
    "# I've decided to include punctuation in my model since it could give interesting clues about the novel (or author)\n",
    "# however I've also decided to ignore some rare symbols - less than 100 occurences\n",
    "# the alphabet to consider doesn't include these symbols\n",
    "alphabet = []\n",
    "\n",
    "for letter in l_hist:\n",
    "    if l_hist[letter]>=100:\n",
    "        alphabet = np.append(alphabet, letter)\n",
    "\n",
    "print(\"For embedding the sentences, we will use an alphabet with %i letters with at least 100 occurences in the training set.\"\n",
    "%len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 1, '\"': 2, \"'\": 3, ')': 4, '(': 5, '-': 6, ',': 7, '.': 8, '1': 9, '0': 10, '3': 11, '2': 12, '5': 13, '8': 14, ';': 15, ':': 16, '?': 17, '[': 18, ']': 19, '_': 20, 'a': 21, 'c': 22, 'b': 23, 'e': 24, 'd': 25, 'g': 26, 'f': 27, 'i': 28, 'h': 29, 'k': 30, 'j': 31, 'm': 32, 'l': 33, 'o': 34, 'n': 35, 'q': 36, 'p': 37, 's': 38, 'r': 39, 'u': 40, 't': 41, 'w': 42, 'v': 43, 'y': 44, 'x': 45, 'z': 46}\n"
     ]
    }
   ],
   "source": [
    "enumeration = dict(enumerate(alphabet,1))\n",
    "alphabet_dict = dict (zip(enumeration.values(),enumeration.keys()))\n",
    "\n",
    "print alphabet_dict\n",
    "            \n",
    "X = np.ones((len(uni_x), max_len), dtype=np.int64) *-1\n",
    "\n",
    "for i, sentence in enumerate(uni_x):\n",
    "    for j, char in enumerate(sentence):\n",
    "        if char in alphabet:\n",
    "            X[i,j] = alphabet_dict[char]\n",
    "            \n",
    "# Note: this code leaves rare symbols as -1, which is also used for padding the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence in the test data has 185 symbols, while the shortest has 87 symbols.\n",
      "The average sentence length is 168.027000 and the standard deviation is 8.201480.\n"
     ]
    }
   ],
   "source": [
    "## loading and preparing of test data \n",
    "with open('./data/xtest.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "test_data = text.split('\\n')\n",
    "test_data = filter(None, test_data)\n",
    "test_data = np.asarray(test_data)\n",
    "uni_test = [sen.encode('ascii', 'ignore') for sen in test_data]\n",
    "\n",
    "test_len = [len(sen) for sen in uni_test]\n",
    "min_len = min(test_len)\n",
    "max_len_test = max(test_len)\n",
    "print \"The longest sentence in the test data has %i symbols, while the shortest has %i symbols.\"%(max_len_test,min_len)\n",
    "print(\"The average sentence length is %f and the standard deviation is %f.\"% (np.mean(test_len),np.std(test_len)))\n",
    "\n",
    "# I'm using the same alphabet as extracted from the training data and the maximum length of the training sentences\n",
    "X_test = np.ones((len(uni_test), np.shape(X)[1]), dtype=np.int64) *-1\n",
    "\n",
    "# we cut the sentences of the test set off since we use the maximum length of the training sentences\n",
    "for i, sentence in enumerate(uni_test):\n",
    "    for j, char in enumerate(sentence):\n",
    "        if char in alphabet and j < len(uni_x):\n",
    "            X_test[i,j] = alphabet_dict[char]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## one-hot encoding\n",
    "# we have to embedd our characters so that we can use them as numerical vectors\n",
    "\n",
    "# technically also other embeddings are available such as char2vec but since of the time constraint, I decided to use one-hot\n",
    "# I think a better embedding could lead to a significantly better model\n",
    "\n",
    "# a depth of 6 should be enough since there are 46 characters in this challenge and 2^6 is 64\n",
    "p_depth = 6\n",
    "\n",
    "# defining this function to call it on the fly\n",
    "def transform_one_hot(x, p_depth=6):\n",
    "    return tf.to_float(tf.one_hot(x, p_depth, on_value=1, off_value=0, axis=-1))\n",
    "\n",
    "def one_hot_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], p_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## data split for validation\n",
    "\n",
    "# split into training and validation from train.txt\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, ydata, test_size=0.05, random_state=0)\n",
    "# since our dataset is already pretty small, we can only afford to have a very small validation set\n",
    "\n",
    "# X_test is kept intact for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model design\n",
    "# I'm aware that at this point the standard model for such a task would be a character-level LSTM (RNN)\n",
    "# However, I've been working on CNN for the past 2 years and have more experience with their design. \n",
    "# A quick literature search confirmed that they can be used for this kind of task\n",
    "# [1]\"Convolutional Neural Networks for Sentence Classification\" (2014, https://arxiv.org/pdf/1408.5882.pdf)\n",
    "# [2]\"A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification\" \n",
    "# (2016, https://arxiv.org/pdf/1510.03820.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## (hyper-)parameters\n",
    "# definition of hyperparameters relevant to this task\n",
    "\n",
    "# dropout rate for regularization\n",
    "p_dropout = 0.1\n",
    "# feature map size\n",
    "# adjusted these values by line search as suggested in paper [2]\n",
    "p_filter_nb1 = [100, 200, 200]\n",
    "p_filter_nb2 = [200, 300, 300]\n",
    "p_filter_nb3 = [300, 400, 400]\n",
    "# filter size\n",
    "# adjusted these values by line search as suggested in paper [2]\n",
    "p_filter_len1 = [5, 3, 3]\n",
    "p_filter_len2 = [11, 3, 3]\n",
    "p_filter_len3 = [15, 3, 3]\n",
    "\n",
    "# Load checkpoint if exists\n",
    "checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Defining some necessary functions\n",
    "# 1d max pooling as recommended in [2]\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "# defining a block\n",
    "def conv_block(in_layer, nb_filter=[100, 100, 100], filter_length=[3, 3, 3], subsample=[1, 1, 1], pool_length=[2, 2, 2]):\n",
    "    block = in_layer\n",
    "    for i in range(len(nb_filter)):\n",
    "\n",
    "        block = Conv1D(nb_filter=nb_filter[i],\n",
    "                              filter_length=filter_length[i],\n",
    "                              border_mode='valid',\n",
    "                              activation='relu',\n",
    "                              subsample_length=subsample[i])(block)\n",
    "        # TODO: dropout or backnorm (experiment!)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Dropout(p_dropout)(block)\n",
    "        if pool_length[i]:\n",
    "            block = MaxPooling1D(pool_length=pool_length[i])(block)\n",
    "\n",
    "    block = Lambda(max_1d, output_shape=(nb_filter[-1],))(block)\n",
    "    block = Dense(128, activation='relu')(block)\n",
    "    block = Dense(128, activation='relu')(block)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The main model in keras\n",
    "# insert the input sentence and embedd to one-hot space on-the-fly\n",
    "in_sentence = Input(shape=(max_len,), dtype='int64')\n",
    "embedded = Lambda(transform_one_hot, output_shape=one_hot_outshape)(in_sentence)\n",
    "\n",
    "# convolve with three blocks\n",
    "block1 = conv_block(embedded, p_filter_nb1, filter_length=p_filter_len1, subsample=[1, 1, 1], pool_length=[2, 2, 2])\n",
    "block2 = conv_block(embedded, p_filter_nb2, filter_length=p_filter_len2, subsample=[1, 1, 1], pool_length=[2, 2, 2])\n",
    "block3 = conv_block(embedded, p_filter_nb3, filter_length=p_filter_len3, subsample=[1, 1, 1], pool_length=[2, 2, 2])\n",
    "\n",
    "# merge blocks to one\n",
    "feature_vec = merge([block1,block2, block3], mode='concat', concat_axis=-1)\n",
    "\n",
    "# added fully connected layer\n",
    "feature_vec = Dropout(p_dropout)(feature_vec)\n",
    "feature_vec = Dense(128, activation='relu')(feature_vec)\n",
    "\n",
    "# softmax classification for 12 classes\n",
    "classes_estimated = Dense(12, activation='softmax')(feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 189)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 189, 6)        0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 185, 100)      3100        lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 179, 200)      13400       lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 175, 300)      27300       lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 185, 100)      400         conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 179, 200)      800         conv1d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 175, 300)      1200        conv1d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 185, 100)      0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 179, 200)      0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 175, 300)      0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 92, 100)       0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)   (None, 89, 200)       0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)   (None, 87, 300)       0           dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 90, 200)       60200       max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 87, 300)       180300      max_pooling1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)                (None, 85, 400)       360400      max_pooling1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 90, 200)       800         conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 87, 300)       1200        conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 85, 400)       1600        conv1d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 90, 200)       0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 87, 300)       0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 85, 400)       0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 45, 200)       0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)   (None, 43, 300)       0           dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)   (None, 42, 400)       0           dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 43, 200)       120200      max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)                (None, 41, 300)       270300      max_pooling1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)                (None, 40, 400)       480400      max_pooling1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 43, 200)       800         conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 41, 300)       1200        conv1d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 40, 400)       1600        conv1d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 43, 200)       0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 41, 300)       0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 40, 400)       0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 21, 200)       0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)   (None, 20, 300)       0           dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)   (None, 20, 400)       0           dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 200)           0           max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 300)           0           max_pooling1d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, 400)           0           max_pooling1d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           25728       lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           38528       lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 128)           51328       lambda_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           16512       dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 128)           16512       dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 128)           16512       dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 384)           0           dense_2[0][0]                    \n",
      "                                                                   dense_4[0][0]                    \n",
      "                                                                   dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 384)           0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 128)           49280       dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 12)            1548        dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1,741,148\n",
      "Trainable params: 1,736,348\n",
      "Non-trainable params: 4,800\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "model = Model(input=in_sentence, output=classes_estimated)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if checkpoint:\n",
    "    model.load_weights(checkpoint_dir)\n",
    "\n",
    "file_name = 'char-level-cnn'\n",
    "check_cb = keras.callbacks.ModelCheckpoint('checkpoints/' + file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                           monitor='val_loss',\n",
    "                                           verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "# TODO: set learning rate decay\n",
    "optimizer = keras.optimizers.Nadam\n",
    "# using loss for multiclass problem\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# following the keras documentation, I've realized that the labels have to be formatted this way in order to use\n",
    "# the categorical cross entropy loss\n",
    "y_train = keras.utils.to_categorical(y_train, 12)\n",
    "y_val = keras.utils.to_categorical(y_val, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31095 samples, validate on 1637 samples\n",
      "Epoch 1/10\n",
      "31095/31095 [==============================] - 432s - loss: 5.3448 - acc: 0.1659 - val_loss: 14.1095 - val_acc: 0.1246\n",
      "Epoch 2/10\n",
      "15872/31095 [==============>...............] - ETA: 206s - loss: 5.3650 - acc: 0.1747"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9b9ef346a646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# the actual training is done here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearlystop_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# the actual training is done here\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), class_weight=class_weights, batch_size=512, epochs=10, shuffle=True, callbacks=[check_cb, earlystop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_val, y_val, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## testing\n",
    "# use model to predict labels for test sentences\n",
    "y_pred = model.predict(X_test)\n",
    "# convert softmax probabilities to class labels\n",
    "y_label = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nETMzK81JxMzMSnMSMTOz0v4/ItDA35BZq7AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b380c0350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity test for labels\n",
    "bins = range(13)\n",
    "hist, bin_edges = np.histogram(y_label, bins, density=False)\n",
    "# shows a nice plot of the histogram of labels\n",
    "plt.title(\"Predicted Label Histogram\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency in percentage\")\n",
    "plt.bar(bin_edges[:-1], hist, width=1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see in this plot that there is actually not an even distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write test sentences to txt file\n",
    "np.savetxt(\"ytest.txt\", y_label, fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
