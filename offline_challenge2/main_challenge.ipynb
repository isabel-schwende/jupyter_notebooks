{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# data preparation imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import normaltest\n",
    "from collections import Counter\n",
    "import re\n",
    "import sklearn as sk\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: cleanup imports\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, GlobalMaxPool1D\n",
    "from keras.layers import merge, Lambda, concatenate, BatchNormalization, Reshape\n",
    "from keras.layers import TimeDistributed, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import keras.callbacks\n",
    "\n",
    "import os\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import x for training, the sentences\n",
    "with open('./data/xtrain.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process data to remove empty line\n",
    "xdata = text.split('\\n')\n",
    "xdata = filter(None, xdata)\n",
    "xdata = np.asarray(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import y for training, the ground truth labels\n",
    "with open('./data/ytrain.txt') as f:\n",
    "    labeltext = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess y to remove emtpy string and convert to integers for checks\n",
    "ydata_str = labeltext.split('\\n')\n",
    "ydata_str = filter(None, ydata_str) \n",
    "\n",
    "ydata = [int(numeric_string) for numeric_string in ydata_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the number of sentences and labels is equal: True\n"
     ]
    }
   ],
   "source": [
    "# verify that both x and y have the same number of entries\n",
    "print \"Checking if the number of sentences and labels is equal:\", len(xdata)==len(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually load the list of book titles for nicer prints\n",
    "# I fixed a minor spelling error in one of the book titles here\n",
    "book_titles = ['alice_in_wonderland','dracula','dubliners','great_expectations','hard_times','huckleberry_finn',\n",
    "               'les_miserable','moby_dick','oliver_twist','peter_pan','tale_of_two_cities','tom_sawyer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sanity check for labels\n",
    "# let's take a look at the labels first\n",
    "# it makes sense to take a look at the labels \n",
    "# I like histograms since it's important to check how our labels are distributed \n",
    "bins = range(13)\n",
    "hist, bin_edges = np.histogram(ydata, bins, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3da41fe0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shows a nice plot of the histogram of labels\n",
    "plt.title(\"Label Histogram\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency in percentage\")\n",
    "plt.bar(bin_edges[:-1], hist, width=1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see in this plot that there is actually not an even distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book alice_in_wonderland, with index 0, has the lowest number of samples, 1.704754 percent.\n",
      "The book moby_dick, with index 7, has the highest number of samples, 15.614689 percent.\n",
      "That means that we have about 9 times more samples from moby_dick than from alice_in_wonderland.\n"
     ]
    }
   ],
   "source": [
    "# print a few interesting numbers about the histogram\n",
    "print (\"The book %s, with index %i, has the lowest number of samples, %f percent.\"\n",
    "%(book_titles[np.argmin(hist)],np.argmin(hist),min(hist)*100) )\n",
    "print (\"The book %s, with index %i, has the highest number of samples, %f percent.\"\n",
    "%(book_titles[np.argmax(hist)],np.argmax(hist),max(hist)*100) )\n",
    "\n",
    "print (\"That means that we have about %i times more samples from %s than from %s.\"\n",
    "%(max(hist)/min(hist),book_titles[np.argmax(hist)],book_titles[np.argmin(hist)]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I've done a few training test runs and realized that the number of labels is screwed so badly that the prediction\n",
    "# didn't work well. For example I obtained this histogram of predicted labels for the test data:\n",
    "# [   0  252    0   72    0    0 2063  434  179    0    0    0] \n",
    "# so some classes were not predicted at all, while only the most common 5 classes were predicted\n",
    "# labels_dict : {ind_label: count_label}\n",
    "\n",
    "def create_class_weight(labels_dict,mu=0.15):\n",
    "    total = np.sum(labels_dict.values())\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Clean up sentences\n",
    "# Some early experiments showed that there were some non-ascii characters in the sentence data\n",
    "# I've decided to remove these\n",
    "uni_x = [sen.encode('ascii', 'ignore') for sen in xdata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence in the training data has 189 symbols, while the shortest has 54 symbols.\n",
      "The average sentence length is 167.874404 and the standard deviation is 8.061963.\n"
     ]
    }
   ],
   "source": [
    "## Sanity check for sentences x\n",
    "x_len = [len(sen) for sen in uni_x]\n",
    "min_len = min(x_len)\n",
    "max_len = max(x_len)\n",
    "print \"The longest sentence in the training data has %i symbols, while the shortest has %i symbols.\"%(max_len,min_len)\n",
    "print(\"The average sentence length is %f and the standard deviation is %f.\"% (np.mean(x_len),np.std(x_len)))\n",
    "\n",
    "# so the length of the sentences varies but not that much, especially the very short sentences seem to be outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    0    0    0    0    1    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    1    2    1    0    0\n",
      "    0    0    0    0    0    0    1    0    0    0    1    0    1    0    0\n",
      "    0    0    0    0    0    0    0    1    0    1    1    0    0    0    0\n",
      "    2    5    0    1    4    3    3    1    2    4    2    4    5    6    6\n",
      "   11   10    6   13   19   18   15   33   23   18   38   43   40   49   64\n",
      "   58   76   90   92  143  144  158  194  196  260  280  313  346  410  430\n",
      "  480  577  645  791  914 1083 1259 1476 1760 1893 2122 2245 2245 2212 2029\n",
      " 1783 1584 1239  904  688  469  286  196   80   59   29   19    8    6]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNcSJw8zMGuLEYWZmDXHiMDOzhvx/4lQmCPupI8AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3da41f7d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = range(min_len,max_len,1)\n",
    "hist, bin_edges = np.histogram(x_len,bins,density=False )\n",
    "print hist\n",
    "\n",
    "# shows a nice plot of the histogram of sentence lengths\n",
    "plt.title(\"Sentence length Histogram\")\n",
    "plt.xlabel(\"Sequence length\")\n",
    "plt.ylabel(\"Frequency absolute numbers\")\n",
    "plt.bar(bin_edges[:-1], hist, width = 1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see in this plot that the sentence length appears to be normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistical test for: p value for normality = 0.000000 .\n"
     ]
    }
   ],
   "source": [
    "print(\"statistical test for: p value for normality = %f .\"%(normaltest(x_len)[1]) )\n",
    "\n",
    "# theory that the data was picked with normally distributed sentence length can't be rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histogram_letters(L):\n",
    "    d = Counter(letter for line in L for letter in line)\n",
    "    for letter in d:\n",
    "        print('{} | {}'.format(letter, d[letter]))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! | 6720\n",
      "# | 3\n",
      "\" | 4520\n",
      "$ | 3\n",
      "' | 10332\n",
      "& | 15\n",
      ") | 804\n",
      "( | 833\n",
      "* | 48\n",
      "- | 19931\n",
      ", | 108481\n",
      "/ | 3\n",
      ". | 50053\n",
      "1 | 364\n",
      "0 | 223\n",
      "3 | 113\n",
      "2 | 159\n",
      "5 | 123\n",
      "4 | 73\n",
      "7 | 96\n",
      "6 | 48\n",
      "9 | 64\n",
      "8 | 159\n",
      "; | 15494\n",
      ": | 2440\n",
      "= | 11\n",
      "< | 6\n",
      "? | 4670\n",
      "> | 6\n",
      "[ | 116\n",
      "] | 103\n",
      "_ | 3365\n",
      "a | 428514\n",
      "c | 118709\n",
      "b | 83080\n",
      "e | 651182\n",
      "d | 242642\n",
      "g | 112092\n",
      "f | 112291\n",
      "i | 357560\n",
      "h | 353582\n",
      "k | 46716\n",
      "j | 8225\n",
      "m | 135069\n",
      "l | 211696\n",
      "o | 402570\n",
      "n | 364930\n",
      "q | 5371\n",
      "p | 85532\n",
      "s | 327557\n",
      "r | 293094\n",
      "u | 147819\n",
      "t | 484089\n",
      "w | 134334\n",
      "v | 46785\n",
      "y | 103313\n",
      "x | 6212\n",
      "{ | 4\n",
      "z | 2515\n",
      "} | 3\n"
     ]
    }
   ],
   "source": [
    "## histogram of the letters\n",
    "l_hist = histogram_letters(uni_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For embedding the sentences, we will use an alphabet with 46 letters with at least 100 occurences in the training set.\n"
     ]
    }
   ],
   "source": [
    "## analysis\n",
    "# 'e', 't' and 'a' are most common letters, which was expectable for a dataset using English language text\n",
    "# I've decided to include punctuation in my model since it could give interesting clues about the novel (or author)\n",
    "# however I've also decided to ignore some rare symbols - less than 100 occurences\n",
    "# the alphabet to consider doesn't include these symbols\n",
    "alphabet = []\n",
    "\n",
    "for letter in l_hist:\n",
    "    if l_hist[letter]>=100:\n",
    "        alphabet = np.append(alphabet, letter)\n",
    "\n",
    "print(\"For embedding the sentences, we will use an alphabet with %i letters with at least 100 occurences in the training set.\"\n",
    "%len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 1, '\"': 2, \"'\": 3, ')': 4, '(': 5, '-': 6, ',': 7, '.': 8, '1': 9, '0': 10, '3': 11, '2': 12, '5': 13, '8': 14, ';': 15, ':': 16, '?': 17, '[': 18, ']': 19, '_': 20, 'a': 21, 'c': 22, 'b': 23, 'e': 24, 'd': 25, 'g': 26, 'f': 27, 'i': 28, 'h': 29, 'k': 30, 'j': 31, 'm': 32, 'l': 33, 'o': 34, 'n': 35, 'q': 36, 'p': 37, 's': 38, 'r': 39, 'u': 40, 't': 41, 'w': 42, 'v': 43, 'y': 44, 'x': 45, 'z': 46}\n"
     ]
    }
   ],
   "source": [
    "enumeration = dict(enumerate(alphabet,1))\n",
    "alphabet_dict = dict (zip(enumeration.values(),enumeration.keys()))\n",
    "\n",
    "print alphabet_dict\n",
    "            \n",
    "X = np.ones((len(uni_x), max_len), dtype=np.int64) *-1\n",
    "\n",
    "for i, sentence in enumerate(uni_x):\n",
    "    for j, char in enumerate(sentence):\n",
    "        if char in alphabet:\n",
    "            X[i,j] = alphabet_dict[char]\n",
    "            \n",
    "# Note: this code leaves rare symbols as -1, which is also used for padding the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence in the test data has 185 symbols, while the shortest has 87 symbols.\n",
      "The average sentence length is 168.027000 and the standard deviation is 8.201480.\n"
     ]
    }
   ],
   "source": [
    "## loading and preparing of test data \n",
    "with open('./data/xtest.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "test_data = text.split('\\n')\n",
    "test_data = filter(None, test_data)\n",
    "test_data = np.asarray(test_data)\n",
    "uni_test = [sen.encode('ascii', 'ignore') for sen in test_data]\n",
    "\n",
    "test_len = [len(sen) for sen in uni_test]\n",
    "min_len = min(test_len)\n",
    "max_len_test = max(test_len)\n",
    "print \"The longest sentence in the test data has %i symbols, while the shortest has %i symbols.\"%(max_len_test,min_len)\n",
    "print(\"The average sentence length is %f and the standard deviation is %f.\"% (np.mean(test_len),np.std(test_len)))\n",
    "\n",
    "# I'm using the same alphabet as extracted from the training data and the maximum length of the training sentences\n",
    "X_test = np.ones((len(uni_test), np.shape(X)[1]), dtype=np.int64) *-1\n",
    "\n",
    "# we cut the sentences of the test set off since we use the maximum length of the training sentences\n",
    "for i, sentence in enumerate(uni_test):\n",
    "    for j, char in enumerate(sentence):\n",
    "        if char in alphabet and j < len(uni_x):\n",
    "            X_test[i,j] = alphabet_dict[char]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## one-hot encoding\n",
    "# we have to embedd our characters so that we can use them as numerical vectors\n",
    "\n",
    "# technically also other embeddings are available such as char2vec but since of the time constraint, I decided to use one-hot\n",
    "# I think a better embedding could lead to a significantly better model\n",
    "\n",
    "# a depth of 64 should be enough since there are 46 characters in this challenge\n",
    "p_depth = 64\n",
    "\n",
    "# defining this function to call it on the fly\n",
    "def transform_one_hot(x, p_depth=64):\n",
    "    return tf.to_float(tf.one_hot(x, p_depth, on_value=1, off_value=0, axis=-1))\n",
    "\n",
    "def one_hot_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], p_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## data split for validation\n",
    "\n",
    "# split into training and validation from train.txt\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, ydata, test_size=0.05, random_state=0)\n",
    "# since our dataset is already pretty small, we can only afford to have a very small validation set\n",
    "\n",
    "# X_test is kept intact for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## resampling\n",
    "# the bias in our data was so big that I decided to resample the training data\n",
    "# this function outputs a binary index vector which samples should be kept for training\n",
    "\n",
    "# sampling ratio can be technically any number but should be greater or equal to 1\n",
    "def sampling_train(y_tr, sampling_ratio=1.0):\n",
    "    bins = range(13)\n",
    "    hist, bin_edges = np.histogram(y_tr, bins, density=False)\n",
    "    # how many samples to take per class - relative to smallest class\n",
    "    n_samples = int(np.min(hist) * sampling_ratio)\n",
    "    \n",
    "    for label in range(len(book_titles)):\n",
    "        idx = np.where(np.equal(y_tr,label))\n",
    "        idx = idx[0]\n",
    "        #shuffle labels\n",
    "        np.random.shuffle(idx)\n",
    "        if n_samples < hist[label]:\n",
    "            idx = idx[:n_samples]\n",
    "        if label == 0:\n",
    "            indices = idx\n",
    "        else:\n",
    "            indices = np.concatenate((indices,idx),axis=0)\n",
    "    np.random.shuffle(indices)\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model design\n",
    "# character-level LSTM, for details please see the model explanation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## (hyper-)parameters\n",
    "# definition of hyperparameters relevant to this task\n",
    "\n",
    "# dropout rate for regularization\n",
    "p_dropout = 0.1\n",
    "\n",
    "lstm_h = 100\n",
    "\n",
    "# Load checkpoint if exists\n",
    "checkpoint = False\n",
    "# check if grid search is wanted\n",
    "grid_search = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 189)               0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 189, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 189, 100)          66000     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 160,876\n",
      "Trainable params: 160,876\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## The main model in keras\n",
    "# insert the input sentence and embedd to one-hot space on-the-fly\n",
    "in_sentence = Input(shape=(max_len,), dtype='int64')\n",
    "embedded = Lambda(transform_one_hot, output_shape=one_hot_outshape)(in_sentence)\n",
    "\n",
    "lstm_layer = LSTM(lstm_h, return_sequences=True, dropout=p_dropout, recurrent_dropout=0.2, implementation=0)(embedded)\n",
    "lstm_layer2 = LSTM(lstm_h, return_sequences=False, dropout=p_dropout, recurrent_dropout=0.2, implementation=0)(lstm_layer)\n",
    "\n",
    "# added fully connected layer\n",
    "feature_vec = Dropout(p_dropout)(lstm_layer2)\n",
    "feature_vec = Dense(128, activation='relu', kernel_initializer='glorot_normal')(feature_vec)\n",
    "\n",
    "# apply dropout for regularization\n",
    "to_out = Dropout(p_dropout)(feature_vec)\n",
    "\n",
    "# output = Dropout(0.2)(bi_lstm)\n",
    "output = Dense(12, activation='softmax', kernel_initializer='lecun_uniform')(to_out)\n",
    "\n",
    "model = Model(outputs=output, inputs=in_sentence)\n",
    "model.summary()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if checkpoint:\n",
    "    model.load_weights(checkpoint_dir)\n",
    "\n",
    "file_name = 'char-level-cnn'\n",
    "check_cb = keras.callbacks.ModelCheckpoint('checkpoints/' + file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                            monitor='val_loss',\n",
    "                                            verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "# TODO: set learning rate decay\n",
    "optimizer = RMSprop(lr=0.001, decay=0.0001)\n",
    "# using loss for multiclass problem\n",
    "# combination of categorical_crossentropy and softmax good for multiclass label problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# following the keras documentation, I've realized that the labels have to be formatted this way in order to use\n",
    "# the categorical cross entropy loss\n",
    "\n",
    "y_val = keras.utils.to_categorical(y_val, 12)\n",
    "#X_val = X_val.reshape(len(X_val), 1, max_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.4804 - acc: 0.0893 - val_loss: 2.4179 - val_acc: 0.1411\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.4549 - acc: 0.1142 - val_loss: 2.4065 - val_acc: 0.1472\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.4466 - acc: 0.1241 - val_loss: 2.4698 - val_acc: 0.1069\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.4418 - acc: 0.1264 - val_loss: 2.4285 - val_acc: 0.1069\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 73s - loss: 2.4378 - acc: 0.1258 - val_loss: 2.4481 - val_acc: 0.1423\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 73s - loss: 2.4388 - acc: 0.1253 - val_loss: 2.4182 - val_acc: 0.1552\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 73s - loss: 2.4384 - acc: 0.1193 - val_loss: 2.4095 - val_acc: 0.1497\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.4332 - acc: 0.1294 - val_loss: 2.4097 - val_acc: 0.1521\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 73s - loss: 2.4341 - acc: 0.1261 - val_loss: 2.4228 - val_acc: 0.1552\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 73s - loss: 2.4344 - acc: 0.1258 - val_loss: 2.4139 - val_acc: 0.1503\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.4254 - acc: 0.1295 - val_loss: 2.4317 - val_acc: 0.1546\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 73s - loss: 2.4225 - acc: 0.1350 - val_loss: 2.3950 - val_acc: 0.1631\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 73s - loss: 2.4188 - acc: 0.1364 - val_loss: 2.4492 - val_acc: 0.1234\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.4159 - acc: 0.1374 - val_loss: 2.3849 - val_acc: 0.1765\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.4115 - acc: 0.1419 - val_loss: 2.3696 - val_acc: 0.1594\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.3978 - acc: 0.1518 - val_loss: 2.3280 - val_acc: 0.1729\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.3926 - acc: 0.1515 - val_loss: 2.3699 - val_acc: 0.1796\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.3907 - acc: 0.1599 - val_loss: 2.3097 - val_acc: 0.1833\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.3805 - acc: 0.1595 - val_loss: 2.3415 - val_acc: 0.1790\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.3668 - acc: 0.1683 - val_loss: 2.2983 - val_acc: 0.1979\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.4135 - acc: 0.1363 - val_loss: 2.3409 - val_acc: 0.1796\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.3233 - acc: 0.1657 - val_loss: 2.2725 - val_acc: 0.1979\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.2926 - acc: 0.1709 - val_loss: 2.3122 - val_acc: 0.1759\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.2769 - acc: 0.1789 - val_loss: 2.1966 - val_acc: 0.1949\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.4666 - acc: 0.1069 - val_loss: 2.5026 - val_acc: 0.0611\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.4596 - acc: 0.1142 - val_loss: 2.4458 - val_acc: 0.1020\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.4681 - acc: 0.1012 - val_loss: 2.4590 - val_acc: 0.1112\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.3470 - acc: 0.1566 - val_loss: 2.2060 - val_acc: 0.2291\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2724 - acc: 0.1811 - val_loss: 2.2213 - val_acc: 0.1936\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2705 - acc: 0.1777 - val_loss: 2.1790 - val_acc: 0.2010\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2564 - acc: 0.1740 - val_loss: 2.1570 - val_acc: 0.2291\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.2683 - acc: 0.1789 - val_loss: 2.3146 - val_acc: 0.1643\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.4430 - acc: 0.1118 - val_loss: 2.3615 - val_acc: 0.1558\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2693 - acc: 0.1767 - val_loss: 2.1693 - val_acc: 0.2211\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.3488 - acc: 0.1646 - val_loss: 2.1561 - val_acc: 0.2321\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.2293 - acc: 0.1885 - val_loss: 2.1418 - val_acc: 0.2389\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2355 - acc: 0.1901 - val_loss: 2.1862 - val_acc: 0.2224\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2575 - acc: 0.1860 - val_loss: 2.2021 - val_acc: 0.1717\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2082 - acc: 0.1902 - val_loss: 2.1699 - val_acc: 0.2211\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.2073 - acc: 0.1923 - val_loss: 2.1446 - val_acc: 0.2486\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.2270 - acc: 0.1860 - val_loss: 2.1883 - val_acc: 0.2077\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2471 - acc: 0.1885 - val_loss: 2.1555 - val_acc: 0.2059\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2063 - acc: 0.1940 - val_loss: 2.1331 - val_acc: 0.2242\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2026 - acc: 0.1938 - val_loss: 2.1202 - val_acc: 0.2266\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2081 - acc: 0.1981 - val_loss: 2.1313 - val_acc: 0.2266\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.1915 - acc: 0.2033 - val_loss: 2.1070 - val_acc: 0.2272\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1890 - acc: 0.2033 - val_loss: 2.1099 - val_acc: 0.2529\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1937 - acc: 0.2019 - val_loss: 2.1269 - val_acc: 0.2187\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1778 - acc: 0.2017 - val_loss: 2.0867 - val_acc: 0.2578\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1756 - acc: 0.2021 - val_loss: 2.0944 - val_acc: 0.2407\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1837 - acc: 0.2068 - val_loss: 2.1560 - val_acc: 0.1894\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1813 - acc: 0.2052 - val_loss: 2.1109 - val_acc: 0.2181\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1702 - acc: 0.2098 - val_loss: 2.0975 - val_acc: 0.2389\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1701 - acc: 0.2093 - val_loss: 2.1519 - val_acc: 0.1985\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1873 - acc: 0.2159 - val_loss: 2.1320 - val_acc: 0.1985\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1587 - acc: 0.2167 - val_loss: 2.1264 - val_acc: 0.2083\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1576 - acc: 0.2139 - val_loss: 2.1057 - val_acc: 0.2248\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1631 - acc: 0.2189 - val_loss: 2.1110 - val_acc: 0.2291\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1463 - acc: 0.2211 - val_loss: 2.0663 - val_acc: 0.2358\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 73s - loss: 2.1552 - acc: 0.2090 - val_loss: 2.1036 - val_acc: 0.2095\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7550/7550 [==============================] - 72s - loss: 2.1544 - acc: 0.2162 - val_loss: 2.0768 - val_acc: 0.2407\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1456 - acc: 0.2238 - val_loss: 2.0841 - val_acc: 0.2395\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1394 - acc: 0.2268 - val_loss: 2.1027 - val_acc: 0.2205\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.1377 - acc: 0.2343 - val_loss: 2.0383 - val_acc: 0.2486\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.1414 - acc: 0.2286 - val_loss: 2.1084 - val_acc: 0.2156\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.1487 - acc: 0.2291 - val_loss: 2.0699 - val_acc: 0.2498\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.1491 - acc: 0.2248 - val_loss: 2.0645 - val_acc: 0.2578\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1249 - acc: 0.2321 - val_loss: 2.0659 - val_acc: 0.2352\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 72s - loss: 2.1313 - acc: 0.2305 - val_loss: 2.0232 - val_acc: 0.2602\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.1373 - acc: 0.2302 - val_loss: 2.1182 - val_acc: 0.2156\n",
      "Train on 7550 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.1389 - acc: 0.2246 - val_loss: 2.0918 - val_acc: 0.2279\n",
      "Epoch 2/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.1282 - acc: 0.2325 - val_loss: 2.0913 - val_acc: 0.2321\n",
      "Epoch 3/5\n",
      "7550/7550 [==============================] - 70s - loss: 2.1277 - acc: 0.2285 - val_loss: 2.0747 - val_acc: 0.2285\n",
      "Epoch 4/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.2096 - acc: 0.2066 - val_loss: 2.2651 - val_acc: 0.1741\n",
      "Epoch 5/5\n",
      "7550/7550 [==============================] - 71s - loss: 2.1675 - acc: 0.2219 - val_loss: 2.0626 - val_acc: 0.2462\n",
      "Test loss: 2.062604,  Test accuracy: 0.246182   ,mu  0.300000\n"
     ]
    }
   ],
   "source": [
    "# the actual training is done here\n",
    "keys = range(0,len(book_titles))\n",
    "bins = range(13)\n",
    "    \n",
    "mu = 0.3\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    #class_weights = create_class_weight(labels_dict,mu)\n",
    "for i in range(15):\n",
    "    # with this split, it's using about half the training samples\n",
    "    indices = sampling_train(y_train,1.2)\n",
    "        \n",
    "    X_sampled = X_train[indices,:]\n",
    "    y_sampled = [y_train[j] for j in indices]\n",
    "        \n",
    "    hist, bin_edges = np.histogram(y_sampled, bins, density=False)\n",
    "        \n",
    "    y_sampled = keras.utils.to_categorical(y_sampled, 12)\n",
    "    #X_sampled = X_sampled.reshape(len(X_sampled), 1, max_len)    \n",
    "       \n",
    "    labels_dict = dict((keys[i],hist[i]) for i in range(len(book_titles)))\n",
    "    class_weights = create_class_weight(labels_dict,mu)\n",
    "        \n",
    "    # class_weight=class_weights,\n",
    "    model.fit(X_sampled, y_sampled, validation_data=(X_val, y_val), batch_size=32, epochs=5, shuffle=True, callbacks=[check_cb, earlystop_cb])\n",
    "score = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"Test loss: %f,  Test accuracy: %f   ,mu  %f\" % (score[0],score[1],mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_1 to have 2 dimensions, but got array with shape (3000, 1, 189)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-72b368c622e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# use model to predict labels for test sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# convert softmax probabilities to class labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1574\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1575\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected input_1 to have 2 dimensions, but got array with shape (3000, 1, 189)"
     ]
    }
   ],
   "source": [
    "## testing\n",
    "# use model to predict labels for test sentences\n",
    "X_test = X_test.reshape(len(X_test), 1, max_len) \n",
    "y_pred = model.predict(X_test)\n",
    "# convert softmax probabilities to class labels\n",
    "y_label = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sanity test for labels\n",
    "hist, bin_edges = np.histogram(y_label, bins, density=False)\n",
    "# shows a nice plot of the histogram of labels\n",
    "plt.title(\"Predicted Label Histogram\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency in percentage\")\n",
    "plt.bar(bin_edges[:-1], hist, width=1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see that the training did not work, the model can not discriminate between the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write test sentences to txt file\n",
    "np.savetxt(\"ytest.txt\", y_label, fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
