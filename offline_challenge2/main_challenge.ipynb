{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# data preparation imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import normaltest\n",
    "from collections import Counter\n",
    "import re\n",
    "import sklearn as sk\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TODO: cleanup imports\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D\n",
    "from keras.layers import merge, Lambda, concatenate, BatchNormalization, Reshape\n",
    "from keras.layers import TimeDistributed, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import keras.callbacks\n",
    "\n",
    "import os\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import x for training, the sentences\n",
    "with open('./data/xtrain.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process data to remove empty line\n",
    "xdata = text.split('\\n')\n",
    "xdata = filter(None, xdata)\n",
    "xdata = np.asarray(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import y for training, the ground truth labels\n",
    "with open('./data/ytrain.txt') as f:\n",
    "    labeltext = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess y to remove emtpy string and convert to integers for checks\n",
    "ydata_str = labeltext.split('\\n')\n",
    "ydata_str = filter(None, ydata_str) \n",
    "\n",
    "ydata = [int(numeric_string) for numeric_string in ydata_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the number of sentences and labels is equal: True\n"
     ]
    }
   ],
   "source": [
    "# verify that both x and y have the same number of entries\n",
    "print \"Checking if the number of sentences and labels is equal:\", len(xdata)==len(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually load the list of book titles for nicer prints\n",
    "# I fixed a minor spelling error in one of the book titles here\n",
    "book_titles = ['alice_in_wonderland','dracula','dubliners','great_expectations','hard_times','huckleberry_finn',\n",
    "               'les_miserable','moby_dick','oliver_twist','peter_pan','tale_of_two_cities','tom_sawyer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sanity check for labels\n",
    "# let's take a look at the labels first\n",
    "# it makes sense to take a look at the labels \n",
    "# I like histograms since it's important to check how our labels are distributed \n",
    "bins = range(13)\n",
    "hist, bin_edges = np.histogram(ydata, bins, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbcf42a1350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shows a nice plot of the histogram of labels\n",
    "plt.title(\"Label Histogram\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency in percentage\")\n",
    "plt.bar(bin_edges[:-1], hist, width=1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see in this plot that there is actually not an even distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book alice_in_wonderland, with index 0, has the lowest number of samples, 1.704754 percent.\n",
      "The book moby_dick, with index 7, has the highest number of samples, 15.614689 percent.\n",
      "That means that we have about 9 times more samples from moby_dick than from alice_in_wonderland.\n"
     ]
    }
   ],
   "source": [
    "# print a few interesting numbers about the histogram\n",
    "print (\"The book %s, with index %i, has the lowest number of samples, %f percent.\"\n",
    "%(book_titles[np.argmin(hist)],np.argmin(hist),min(hist)*100) )\n",
    "print (\"The book %s, with index %i, has the highest number of samples, %f percent.\"\n",
    "%(book_titles[np.argmax(hist)],np.argmax(hist),max(hist)*100) )\n",
    "\n",
    "print (\"That means that we have about %i times more samples from %s than from %s.\"\n",
    "%(max(hist)/min(hist),book_titles[np.argmax(hist)],book_titles[np.argmin(hist)]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Clean up sentences\n",
    "# Some early experiments showed that there were some non-ascii characters in the sentence data\n",
    "# I've decided to remove these\n",
    "uni_x = [sen.encode('ascii', 'ignore') for sen in xdata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence in the training data has 189 symbols, while the shortest has 54 symbols.\n",
      "The average sentence length is 167.874404 and the standard deviation is 8.061963.\n"
     ]
    }
   ],
   "source": [
    "## Sanity check for sentences x\n",
    "x_len = [len(sen) for sen in uni_x]\n",
    "min_len = min(x_len)\n",
    "max_len = max(x_len)\n",
    "print \"The longest sentence in the training data has %i symbols, while the shortest has %i symbols.\"%(max_len,min_len)\n",
    "print(\"The average sentence length is %f and the standard deviation is %f.\"% (np.mean(x_len),np.std(x_len)))\n",
    "\n",
    "# so the length of the sentences varies but not that much, especially the very short sentences seem to be outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    0    0    0    0    1    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    1    2    1    0    0\n",
      "    0    0    0    0    0    0    1    0    0    0    1    0    1    0    0\n",
      "    0    0    0    0    0    0    0    1    0    1    1    0    0    0    0\n",
      "    2    5    0    1    4    3    3    1    2    4    2    4    5    6    6\n",
      "   11   10    6   13   19   18   15   33   23   18   38   43   40   49   64\n",
      "   58   76   90   92  143  144  158  194  196  260  280  313  346  410  430\n",
      "  480  577  645  791  914 1083 1259 1476 1760 1893 2122 2245 2245 2212 2029\n",
      " 1783 1584 1239  904  688  469  286  196   80   59   29   19    8    6]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNcSJw8zMGuLEYWZmDXHiMDOzhvx/4lQmCPupI8AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc88ed2d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = range(min_len,max_len,1)\n",
    "hist, bin_edges = np.histogram(x_len,bins,density=False )\n",
    "print hist\n",
    "\n",
    "# shows a nice plot of the histogram of sentence lengths\n",
    "plt.title(\"Sentence length Histogram\")\n",
    "plt.xlabel(\"Sequence length\")\n",
    "plt.ylabel(\"Frequency absolute numbers\")\n",
    "plt.bar(bin_edges[:-1], hist, width = 1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see in this plot that the sentence length appears to be normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistical test for: p value for normality = 0.000000 .\n"
     ]
    }
   ],
   "source": [
    "print(\"statistical test for: p value for normality = %f .\"%(normaltest(x_len)[1]) )\n",
    "\n",
    "# theory that the data was picked with normally distributed sentence length can't be rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histogram_letters(L):\n",
    "    d = Counter(letter for line in L for letter in line)\n",
    "    for letter in d:\n",
    "        print('{} | {}'.format(letter, d[letter]))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! | 6720\n",
      "# | 3\n",
      "\" | 4520\n",
      "$ | 3\n",
      "' | 10332\n",
      "& | 15\n",
      ") | 804\n",
      "( | 833\n",
      "* | 48\n",
      "- | 19931\n",
      ", | 108481\n",
      "/ | 3\n",
      ". | 50053\n",
      "1 | 364\n",
      "0 | 223\n",
      "3 | 113\n",
      "2 | 159\n",
      "5 | 123\n",
      "4 | 73\n",
      "7 | 96\n",
      "6 | 48\n",
      "9 | 64\n",
      "8 | 159\n",
      "; | 15494\n",
      ": | 2440\n",
      "= | 11\n",
      "< | 6\n",
      "? | 4670\n",
      "> | 6\n",
      "[ | 116\n",
      "] | 103\n",
      "_ | 3365\n",
      "a | 428514\n",
      "c | 118709\n",
      "b | 83080\n",
      "e | 651182\n",
      "d | 242642\n",
      "g | 112092\n",
      "f | 112291\n",
      "i | 357560\n",
      "h | 353582\n",
      "k | 46716\n",
      "j | 8225\n",
      "m | 135069\n",
      "l | 211696\n",
      "o | 402570\n",
      "n | 364930\n",
      "q | 5371\n",
      "p | 85532\n",
      "s | 327557\n",
      "r | 293094\n",
      "u | 147819\n",
      "t | 484089\n",
      "w | 134334\n",
      "v | 46785\n",
      "y | 103313\n",
      "x | 6212\n",
      "{ | 4\n",
      "z | 2515\n",
      "} | 3\n"
     ]
    }
   ],
   "source": [
    "## histogram of the letters\n",
    "l_hist = histogram_letters(uni_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For embedding the sentences, we will use an alphabet with 46 letters with at least 100 occurences in the training set.\n"
     ]
    }
   ],
   "source": [
    "## analysis\n",
    "# 'e', 't' and 'a' are most common letters, which was expectable for a dataset using English language text\n",
    "# I've decided to include punctuation in my model since it could give interesting clues about the novel (or author)\n",
    "# however I've also decided to ignore some rare symbols - less than 100 occurences\n",
    "# the alphabet to consider doesn't include these symbols\n",
    "alphabet = []\n",
    "\n",
    "for letter in l_hist:\n",
    "    if l_hist[letter]>=100:\n",
    "        alphabet = np.append(alphabet, letter)\n",
    "\n",
    "print(\"For embedding the sentences, we will use an alphabet with %i letters with at least 100 occurences in the training set.\"\n",
    "%len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 1, '\"': 2, \"'\": 3, ')': 4, '(': 5, '-': 6, ',': 7, '.': 8, '1': 9, '0': 10, '3': 11, '2': 12, '5': 13, '8': 14, ';': 15, ':': 16, '?': 17, '[': 18, ']': 19, '_': 20, 'a': 21, 'c': 22, 'b': 23, 'e': 24, 'd': 25, 'g': 26, 'f': 27, 'i': 28, 'h': 29, 'k': 30, 'j': 31, 'm': 32, 'l': 33, 'o': 34, 'n': 35, 'q': 36, 'p': 37, 's': 38, 'r': 39, 'u': 40, 't': 41, 'w': 42, 'v': 43, 'y': 44, 'x': 45, 'z': 46}\n"
     ]
    }
   ],
   "source": [
    "## Change from characters to integer encoding\n",
    "enumeration = dict(enumerate(alphabet,1))\n",
    "alphabet_dict = dict (zip(enumeration.values(),enumeration.keys()))\n",
    "\n",
    "print alphabet_dict\n",
    "            \n",
    "X = np.ones((len(uni_x), max_len), dtype=np.int64) *-1\n",
    "\n",
    "for i, sentence in enumerate(uni_x):\n",
    "    for j, char in enumerate(sentence):\n",
    "        if char in alphabet:\n",
    "            X[i,j] = alphabet_dict[char]\n",
    "            \n",
    "# Note: this code leaves rare symbols as -1, which is also used for padding the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence in the test data has 185 symbols, while the shortest has 87 symbols.\n",
      "The average sentence length is 168.027000 and the standard deviation is 8.201480.\n"
     ]
    }
   ],
   "source": [
    "## loading and preparing of test data \n",
    "with open('./data/xtest.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "test_data = text.split('\\n')\n",
    "test_data = filter(None, test_data)\n",
    "test_data = np.asarray(test_data)\n",
    "uni_test = [sen.encode('ascii', 'ignore') for sen in test_data]\n",
    "\n",
    "test_len = [len(sen) for sen in uni_test]\n",
    "min_len = min(test_len)\n",
    "max_len_test = max(test_len)\n",
    "print \"The longest sentence in the test data has %i symbols, while the shortest has %i symbols.\"%(max_len_test,min_len)\n",
    "print(\"The average sentence length is %f and the standard deviation is %f.\"% (np.mean(test_len),np.std(test_len)))\n",
    "\n",
    "# I'm using the same alphabet as extracted from the training data and the maximum length of the training sentences\n",
    "X_test = np.ones((len(uni_test), np.shape(X)[1]), dtype=np.int64) *-1\n",
    "\n",
    "# we cut the sentences of the test set off since we use the maximum length of the training sentences\n",
    "for i, sentence in enumerate(uni_test):\n",
    "    for j, char in enumerate(sentence):\n",
    "        if char in alphabet and j < len(uni_x):\n",
    "            X_test[i,j] = alphabet_dict[char]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## one-hot encoding\n",
    "# we have to embedd our characters so that we can use them as numerical vectors\n",
    "\n",
    "# technically also other embeddings are available such as char2vec but since of the time constraint, I decided to use one-hot\n",
    "# I think a better embedding could lead to a significantly better model\n",
    "\n",
    "# a depth of 64 should be enough since there are 46 characters plus padding and rare symbol token\n",
    "p_depth = 64\n",
    "\n",
    "# defining this function to call it on the fly\n",
    "def transform_one_hot(x, p_depth=64):\n",
    "    return tf.to_float(tf.one_hot(x, p_depth, on_value=1, off_value=0, axis=-1))\n",
    "\n",
    "def one_hot_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], p_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## data split for validation\n",
    "\n",
    "# split into training and validation from train.txt\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, ydata, test_size=0.05, random_state=0)\n",
    "# since our dataset is already pretty small, we can only afford to have a very small validation set\n",
    "\n",
    "# X_test is kept intact for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I've done a few training test runs and realized that the number of labels is screwed so badly that the prediction\n",
    "# didn't work well. For example I obtained this histogram of predicted labels for the test data:\n",
    "# [   0   0    0    0    0    0   3000    0    0    0    0    0] \n",
    "# so only one class was picked as prediction\n",
    "# one possibility to counter this problem is to introduce weights for the loss function\n",
    "\n",
    "def create_class_weight(labels_dict,mu=0.15):\n",
    "    total = np.sum(labels_dict.values())\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## resampling\n",
    "# the bias in our data was so big that I decided to resample the training data\n",
    "# this function outputs a binary index vector which samples should be kept for training\n",
    "\n",
    "# sampling ratio can be technically any number but should be greater or equal to 1\n",
    "def sampling_train(y_tr, sampling_ratio=1.0):\n",
    "    bins = range(13)\n",
    "    hist, bin_edges = np.histogram(y_tr, bins, density=False)\n",
    "    # how many samples to take per class - relative to smallest class\n",
    "    n_samples = int(np.min(hist) * sampling_ratio)\n",
    "    \n",
    "    for label in range(len(book_titles)):\n",
    "        idx = np.where(np.equal(y_tr,label))\n",
    "        idx = idx[0]\n",
    "        #shuffle labels\n",
    "        np.random.shuffle(idx)\n",
    "        if n_samples < hist[label]:\n",
    "            idx = idx[:n_samples]\n",
    "        if label == 0:\n",
    "            indices = idx\n",
    "        else:\n",
    "            indices = np.concatenate((indices,idx),axis=0)\n",
    "    np.random.shuffle(indices)\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model design\n",
    "# please find the details on this on the separate txt document on the model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## (hyper-)parameters\n",
    "# definition of hyperparameters relevant to this task\n",
    "\n",
    "# dropout rate for regularization\n",
    "p_dropout = 0.1\n",
    "# feature map size\n",
    "# adjusted these values by line search as suggested in paper [2]\n",
    "p_filter_nb1 = [100, 200, 200]\n",
    "p_filter_nb2 = [200, 300, 300]\n",
    "p_filter_nb3 = [300, 400, 400]\n",
    "# filter size\n",
    "# adjusted these values by line search as suggested in paper [2]\n",
    "p_filter_len1 = [5, 3, 3]\n",
    "p_filter_len2 = [11, 5, 3]\n",
    "p_filter_len3 = [19, 7, 3]\n",
    "\n",
    "# Load checkpoint if exists\n",
    "checkpoint = False\n",
    "# check if grid search is wanted\n",
    "grid_search = False\n",
    "\n",
    "# number of LSTM units\n",
    "lstm_h = 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Defining some necessary functions\n",
    "# 1d max pooling as recommended in [2]\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "# defining a block\n",
    "def conv_block(in_layer, nb_filter=[100, 100, 100], filter_length=[3, 3, 3], subsample=[1, 1, 1], pool_length=[2, 2, 2]):\n",
    "    block = in_layer\n",
    "    for i in range(len(nb_filter)):\n",
    "\n",
    "        block = Conv1D(nb_filter=nb_filter[i],\n",
    "                              filter_length=filter_length[i],\n",
    "                              border_mode='valid',\n",
    "                              activation='relu',\n",
    "                              kernel_initializer='glorot_normal',\n",
    "                              subsample_length=subsample[i])(block)\n",
    "        # TODO: dropout or backnorm (experiment!)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Dropout(p_dropout)(block)\n",
    "        if pool_length[i]:\n",
    "            block = MaxPooling1D(pool_length=pool_length[i])(block)\n",
    "\n",
    "    block = Lambda(max_1d, output_shape=(nb_filter[-1],))(block)\n",
    "    block = Dense(128, activation='relu', kernel_initializer='glorot_normal')(block)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 189)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 189, 64)       0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 185, 100)      32100       lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 179, 200)      141000      lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 171, 300)      365100      lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 185, 100)      400         conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 179, 200)      800         conv1d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 171, 300)      1200        conv1d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 185, 100)      0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 179, 200)      0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 171, 300)      0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 92, 100)       0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)   (None, 89, 200)       0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)   (None, 85, 300)       0           dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 90, 200)       60200       max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 85, 300)       300300      max_pooling1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)                (None, 79, 400)       840400      max_pooling1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 90, 200)       800         conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 85, 300)       1200        conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 79, 400)       1600        conv1d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 90, 200)       0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 85, 300)       0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 79, 400)       0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 45, 200)       0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)   (None, 42, 300)       0           dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)   (None, 39, 400)       0           dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 43, 200)       120200      max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)                (None, 40, 300)       270300      max_pooling1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)                (None, 37, 400)       480400      max_pooling1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 43, 200)       800         conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 40, 300)       1200        conv1d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 37, 400)       1600        conv1d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 43, 200)       0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 40, 300)       0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 37, 400)       0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 21, 200)       0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)   (None, 20, 300)       0           dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)   (None, 18, 400)       0           dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 200)           0           max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 300)           0           max_pooling1d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, 400)           0           max_pooling1d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           25728       lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           38528       lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           51328       lambda_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 384)           0           dense_1[0][0]                    \n",
      "                                                                   dense_2[0][0]                    \n",
      "                                                                   dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 384)           0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 128)           49280       dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 128)           0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2,784,464\n",
      "Trainable params: 2,779,664\n",
      "Non-trainable params: 4,800\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## The main model in keras\n",
    "# insert the input sentence and embedd to one-hot space on-the-fly\n",
    "in_sentence = Input(shape=(max_len,), dtype='int64')\n",
    "embedded = Lambda(transform_one_hot, output_shape=one_hot_outshape)(in_sentence)\n",
    "\n",
    "# convolve with three blocks\n",
    "block1 = conv_block(embedded, p_filter_nb1, filter_length=p_filter_len1, subsample=[1, 1, 1], pool_length=[2, 2, 2])\n",
    "block2 = conv_block(embedded, p_filter_nb2, filter_length=p_filter_len2, subsample=[1, 1, 1], pool_length=[2, 2, 2])\n",
    "block3 = conv_block(embedded, p_filter_nb3, filter_length=p_filter_len3, subsample=[1, 1, 1], pool_length=[2, 2, 2])\n",
    "\n",
    "# merge blocks to one\n",
    "feature_vec = merge([block1,block2, block3], mode='concat', concat_axis=-1)\n",
    "\n",
    "# added fully connected layer\n",
    "feature_vec = Dropout(p_dropout)(feature_vec)\n",
    "feature_vec = Dense(128, activation='relu', kernel_initializer='glorot_normal')(feature_vec)\n",
    "\n",
    "# send feature vector to encoder\n",
    "to_encode = Dropout(p_dropout)(feature_vec)\n",
    "encoder = Model(input=in_sentence, output=to_encode)\n",
    "    \n",
    "encoder.summary()   \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1, 189)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 128)            2784464   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 92)             81328     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 92)                68080     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                1116      \n",
      "=================================================================\n",
      "Total params: 2,934,988\n",
      "Trainable params: 2,930,188\n",
      "Non-trainable params: 4,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## LSTM classifier\n",
    "# LSTM units on top of the convolution feature extractor\n",
    "book_sentences = Input(shape=(1,max_len), dtype='int64')\n",
    "encoded = TimeDistributed(encoder)(book_sentences)\n",
    "\n",
    "lstm_layer = LSTM(lstm_h, return_sequences=True, dropout=0.1, recurrent_dropout=0.1, implementation=0)(encoded)\n",
    "lstm_layer2 = LSTM(lstm_h, return_sequences=False, dropout=0.1, recurrent_dropout=0.1, implementation=0)(lstm_layer)\n",
    "\n",
    "output = Dense(12, activation='softmax', kernel_initializer='lecun_uniform')(lstm_layer2)\n",
    "\n",
    "model = Model(outputs=output, inputs=book_sentences)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if checkpoint:\n",
    "    model.load_weights(checkpoint_dir)\n",
    "\n",
    "file_name = 'char-level-cnn-lstm'\n",
    "check_cb = keras.callbacks.ModelCheckpoint('checkpoints/' + file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                            monitor='val_loss',\n",
    "                                            verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "# set learning rate decay low\n",
    "optimizer = RMSprop(lr=0.01, decay=0.001)\n",
    "# using loss for multiclass problem\n",
    "# combination of categorical_crossentropy and softmax good for multiclass label problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# following the keras documentation, I've realized that the labels have to be formatted this way in order to use\n",
    "# the categorical cross entropy loss\n",
    "\n",
    "y_val = keras.utils.to_categorical(y_val, len(book_titles))\n",
    "X_val = X_val.reshape(len(X_val), 1, max_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12125 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "12125/12125 [==============================] - 265s - loss: 2.5069 - acc: 0.0892 - val_loss: 2.4551 - val_acc: 0.0971\n",
      "Epoch 2/5\n",
      "12125/12125 [==============================] - 268s - loss: 2.5051 - acc: 0.0819 - val_loss: 2.4553 - val_acc: 0.1246\n",
      "Epoch 3/5\n",
      "12125/12125 [==============================] - 266s - loss: 2.5048 - acc: 0.0851 - val_loss: 2.4647 - val_acc: 0.0971\n",
      "Epoch 4/5\n",
      "12125/12125 [==============================] - 266s - loss: 2.5041 - acc: 0.0896 - val_loss: 2.4653 - val_acc: 0.0458\n",
      "Epoch 5/5\n",
      "12125/12125 [==============================] - 264s - loss: 2.5041 - acc: 0.0908 - val_loss: 2.4491 - val_acc: 0.1564\n",
      "Train on 12125 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "12125/12125 [==============================] - 266s - loss: 2.5039 - acc: 0.0864 - val_loss: 2.4570 - val_acc: 0.1045\n",
      "Epoch 2/5\n",
      "12125/12125 [==============================] - 265s - loss: 2.5039 - acc: 0.0862 - val_loss: 2.4552 - val_acc: 0.0971\n",
      "Epoch 3/5\n",
      "12125/12125 [==============================] - 261s - loss: 2.5038 - acc: 0.0809 - val_loss: 2.4581 - val_acc: 0.0458\n",
      "Epoch 4/5\n",
      "12125/12125 [==============================] - 263s - loss: 2.5037 - acc: 0.0858 - val_loss: 2.4516 - val_acc: 0.1246\n",
      "Epoch 5/5\n",
      "12125/12125 [==============================] - 265s - loss: 2.5038 - acc: 0.0846 - val_loss: 2.4571 - val_acc: 0.0611\n",
      "Train on 12125 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "12125/12125 [==============================] - 264s - loss: 2.5033 - acc: 0.0842 - val_loss: 2.4552 - val_acc: 0.1564\n",
      "Epoch 2/5\n",
      "12125/12125 [==============================] - 264s - loss: 2.5037 - acc: 0.0804 - val_loss: 2.4547 - val_acc: 0.0446\n",
      "Epoch 3/5\n",
      "12125/12125 [==============================] - 264s - loss: 2.5035 - acc: 0.0821 - val_loss: 2.4510 - val_acc: 0.1564\n",
      "Epoch 4/5\n",
      "12125/12125 [==============================] - 263s - loss: 2.5034 - acc: 0.0882 - val_loss: 2.4528 - val_acc: 0.1045\n",
      "Epoch 5/5\n",
      "12125/12125 [==============================] - 264s - loss: 2.5033 - acc: 0.0849 - val_loss: 2.4547 - val_acc: 0.1564\n",
      "Train on 12125 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "12125/12125 [==============================] - 265s - loss: 2.5034 - acc: 0.0827 - val_loss: 2.4535 - val_acc: 0.0971\n",
      "Epoch 2/5\n",
      "12125/12125 [==============================] - 265s - loss: 2.5033 - acc: 0.0865 - val_loss: 2.4568 - val_acc: 0.0971\n",
      "Epoch 3/5\n",
      "12125/12125 [==============================] - 262s - loss: 2.5035 - acc: 0.0854 - val_loss: 2.4615 - val_acc: 0.0458\n",
      "Epoch 4/5\n",
      "12125/12125 [==============================] - 262s - loss: 2.5034 - acc: 0.0850 - val_loss: 2.4562 - val_acc: 0.0458\n",
      "Epoch 5/5\n",
      "12125/12125 [==============================] - 261s - loss: 2.5034 - acc: 0.0813 - val_loss: 2.4549 - val_acc: 0.0446\n",
      "Train on 12125 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "12125/12125 [==============================] - 264s - loss: 2.5033 - acc: 0.0822 - val_loss: 2.4562 - val_acc: 0.0446\n",
      "Epoch 2/5\n",
      "12125/12125 [==============================] - 262s - loss: 2.5033 - acc: 0.0868 - val_loss: 2.4563 - val_acc: 0.0446\n",
      "Epoch 3/5\n",
      "12125/12125 [==============================] - 261s - loss: 2.5032 - acc: 0.0876 - val_loss: 2.4563 - val_acc: 0.0971\n",
      "Epoch 4/5\n",
      "12125/12125 [==============================] - 262s - loss: 2.5030 - acc: 0.0836 - val_loss: 2.4566 - val_acc: 0.1100\n",
      "Epoch 5/5\n",
      "12125/12125 [==============================] - 263s - loss: 2.5034 - acc: 0.0803 - val_loss: 2.4546 - val_acc: 0.0971\n",
      "Test loss: 2.454644,  Test accuracy: 0.097129   ,mu  0.150000\n"
     ]
    }
   ],
   "source": [
    "# the actual training is done here\n",
    "# it ended up being a bit ugly since I wanted to adjust the imbalanced labels and I also used it for searching for good hyperparameters\n",
    "keys = range(0,len(book_titles))\n",
    "bins = range(len(book_titles)+1)\n",
    "    \n",
    "if grid_search:\n",
    "    # grid search for good mu parameter for class weights\n",
    "    mu = np.linspace(0.001,1.0,20)\n",
    "    y_train = keras.utils.to_categorical(y_train, len(book_titles))\n",
    "    X_train = X_train.reshape(len(X_train), 1, max_len)   \n",
    "    \n",
    "    hist, bin_edges = np.histogram(ydata, bins, density=False)\n",
    "    \n",
    "    labels_dict = dict((keys[i],hist[i]) for i in range(len(book_titles)))\n",
    "    \n",
    "    for i in mu:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        class_weights = create_class_weight(labels_dict,i)\n",
    "        # the actual training is done here\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val), class_weight=class_weights, batch_size=512, epochs=10, verbose=1, shuffle=True, callbacks=[check_cb, earlystop_cb])\n",
    "        score = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(\"Test loss: %f,  Test accuracy: %f   ,mu  %f\" % (score[0],score[1],i))\n",
    "else:\n",
    "    # mu is fixed here\n",
    "    mu = 0.15\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    for i in range(5):\n",
    "        # with this split, it's using about an equal number of training samples from each class\n",
    "        indices = sampling_train(y_train,2.0)\n",
    "        \n",
    "        X_sampled = X_train[indices,:]\n",
    "        y_sampled = [y_train[j] for j in indices]\n",
    "        \n",
    "        hist, bin_edges = np.histogram(y_sampled, bins, density=False)\n",
    "        \n",
    "        y_sampled = keras.utils.to_categorical(y_sampled, len(book_titles))\n",
    "        X_sampled = X_sampled.reshape(len(X_sampled), 1, max_len)    \n",
    "       \n",
    "        labels_dict = dict((keys[i],hist[i]) for i in range(len(book_titles)))\n",
    "        class_weights = create_class_weight(labels_dict,mu)\n",
    "        #\n",
    "        \n",
    "        model.fit(X_sampled, y_sampled, validation_data=(X_val, y_val), class_weight=class_weights, batch_size=64, epochs=5, shuffle=True, callbacks=[check_cb, earlystop_cb])\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(\"Test loss: %f,  Test accuracy: %f   ,mu  %f\" % (score[0],score[1],mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## testing\n",
    "# use model to predict labels for test sentences\n",
    "X_test = X_test.reshape(len(X_test), 1, max_len) \n",
    "y_pred = model.predict(X_test)\n",
    "# convert softmax probabilities to class labels\n",
    "y_label = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7PSnETMzKw0JxEzMyvt/wPhGcDflnkp3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc78721690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity test for labels\n",
    "hist, bin_edges = np.histogram(y_label, bins, density=False)\n",
    "# shows a nice plot of the histogram of labels\n",
    "plt.title(\"Predicted Label Histogram\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency in percentage\")\n",
    "plt.bar(bin_edges[:-1], hist, width=1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see that the training did not work, the model can not discriminate between the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write test sentences to txt file\n",
    "np.savetxt(\"./data/ytest.txt\", y_label, fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
