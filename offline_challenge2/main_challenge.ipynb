{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# data preparation imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import normaltest\n",
    "from collections import Counter\n",
    "import re\n",
    "import sklearn as sk\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TODO: cleanup imports\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D, GlobalMaxPool1D\n",
    "from keras.layers import merge, Lambda, concatenate, BatchNormalization, Reshape\n",
    "from keras.layers import TimeDistributed, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import keras.callbacks\n",
    "\n",
    "import os\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import x for training, the sentences\n",
    "with open('./data/xtrain.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process data to remove empty line\n",
    "xdata = text.split('\\n')\n",
    "xdata = filter(None, xdata)\n",
    "xdata = np.asarray(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import y for training, the ground truth labels\n",
    "with open('./data/ytrain.txt') as f:\n",
    "    labeltext = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess y to remove emtpy string and convert to integers for checks\n",
    "ydata_str = labeltext.split('\\n')\n",
    "ydata_str = filter(None, ydata_str) \n",
    "\n",
    "ydata = [int(numeric_string) for numeric_string in ydata_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the number of sentences and labels is equal: True\n"
     ]
    }
   ],
   "source": [
    "# verify that both x and y have the same number of entries\n",
    "print \"Checking if the number of sentences and labels is equal:\", len(xdata)==len(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually load the list of book titles for nicer prints\n",
    "# I fixed a minor spelling error in one of the book titles here\n",
    "book_titles = ['alice_in_wonderland','dracula','dubliners','great_expectations','hard_times','huckleberry_finn',\n",
    "               'les_miserable','moby_dick','oliver_twist','peter_pan','tale_of_two_cities','tom_sawyer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sanity check for labels\n",
    "# let's take a look at the labels first\n",
    "# it makes sense to take a look at the labels \n",
    "# I like histograms since it's important to check how our labels are distributed \n",
    "bins = range(13)\n",
    "hist, bin_edges = np.histogram(ydata, bins, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f62ac45f1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shows a nice plot of the histogram of labels\n",
    "plt.title(\"Label Histogram\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency in percentage\")\n",
    "plt.bar(bin_edges[:-1], hist, width=1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see in this plot that there is actually not an even distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book alice_in_wonderland, with index 0, has the lowest number of samples, 1.704754 percent.\n",
      "The book moby_dick, with index 7, has the highest number of samples, 15.614689 percent.\n",
      "That means that we have about 9 times more samples from moby_dick than from alice_in_wonderland.\n"
     ]
    }
   ],
   "source": [
    "# print a few interesting numbers about the histogram\n",
    "print (\"The book %s, with index %i, has the lowest number of samples, %f percent.\"\n",
    "%(book_titles[np.argmin(hist)],np.argmin(hist),min(hist)*100) )\n",
    "print (\"The book %s, with index %i, has the highest number of samples, %f percent.\"\n",
    "%(book_titles[np.argmax(hist)],np.argmax(hist),max(hist)*100) )\n",
    "\n",
    "print (\"That means that we have about %i times more samples from %s than from %s.\"\n",
    "%(max(hist)/min(hist),book_titles[np.argmax(hist)],book_titles[np.argmin(hist)]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've done a few training test runs and realized that the number of labels is screwed so badly that the prediction\n",
    "# didn't work well. For example I obtained this histogram of predicted labels for the test data:\n",
    "# [   0  252    0   72    0    0 2063  434  179    0    0    0] \n",
    "# so some classes were not predicted at all, while only the most common 5 classes were predicted\n",
    "# labels_dict : {ind_label: count_label}\n",
    "\n",
    "def create_class_weight(labels_dict,mu=0.15):\n",
    "    total = np.sum(labels_dict.values())\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Clean up sentences\n",
    "# Some early experiments showed that there were some non-ascii characters in the sentence data\n",
    "# I've decided to remove these\n",
    "uni_x = [sen.encode('ascii', 'ignore') for sen in xdata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence in the training data has 189 symbols, while the shortest has 54 symbols.\n",
      "The average sentence length is 167.874404 and the standard deviation is 8.061963.\n"
     ]
    }
   ],
   "source": [
    "## Sanity check for sentences x\n",
    "x_len = [len(sen) for sen in uni_x]\n",
    "min_len = min(x_len)\n",
    "max_len = max(x_len)\n",
    "print \"The longest sentence in the training data has %i symbols, while the shortest has %i symbols.\"%(max_len,min_len)\n",
    "print(\"The average sentence length is %f and the standard deviation is %f.\"% (np.mean(x_len),np.std(x_len)))\n",
    "\n",
    "# so the length of the sentences varies but not that much, especially the very short sentences seem to be outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    0    0    0    0    1    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    1    2    1    0    0\n",
      "    0    0    0    0    0    0    1    0    0    0    1    0    1    0    0\n",
      "    0    0    0    0    0    0    0    1    0    1    1    0    0    0    0\n",
      "    2    5    0    1    4    3    3    1    2    4    2    4    5    6    6\n",
      "   11   10    6   13   19   18   15   33   23   18   38   43   40   49   64\n",
      "   58   76   90   92  143  144  158  194  196  260  280  313  346  410  430\n",
      "  480  577  645  791  914 1083 1259 1476 1760 1893 2122 2245 2245 2212 2029\n",
      " 1783 1584 1239  904  688  469  286  196   80   59   29   19    8    6]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNcSJw8zMGuLEYWZmDXHiMDOzhvx/4lQmCPupI8AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6241853cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = range(min_len,max_len,1)\n",
    "hist, bin_edges = np.histogram(x_len,bins,density=False )\n",
    "print hist\n",
    "\n",
    "# shows a nice plot of the histogram of sentence lengths\n",
    "plt.title(\"Sentence length Histogram\")\n",
    "plt.xlabel(\"Sequence length\")\n",
    "plt.ylabel(\"Frequency absolute numbers\")\n",
    "plt.bar(bin_edges[:-1], hist, width = 1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see in this plot that the sentence length appears to be normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistical test for: p value for normality = 0.000000 .\n"
     ]
    }
   ],
   "source": [
    "print(\"statistical test for: p value for normality = %f .\"%(normaltest(x_len)[1]) )\n",
    "\n",
    "# theory that the data was picked with normally distributed sentence length can't be rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histogram_letters(L):\n",
    "    d = Counter(letter for line in L for letter in line)\n",
    "    for letter in d:\n",
    "        print('{} | {}'.format(letter, d[letter]))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! | 6720\n",
      "# | 3\n",
      "\" | 4520\n",
      "$ | 3\n",
      "' | 10332\n",
      "& | 15\n",
      ") | 804\n",
      "( | 833\n",
      "* | 48\n",
      "- | 19931\n",
      ", | 108481\n",
      "/ | 3\n",
      ". | 50053\n",
      "1 | 364\n",
      "0 | 223\n",
      "3 | 113\n",
      "2 | 159\n",
      "5 | 123\n",
      "4 | 73\n",
      "7 | 96\n",
      "6 | 48\n",
      "9 | 64\n",
      "8 | 159\n",
      "; | 15494\n",
      ": | 2440\n",
      "= | 11\n",
      "< | 6\n",
      "? | 4670\n",
      "> | 6\n",
      "[ | 116\n",
      "] | 103\n",
      "_ | 3365\n",
      "a | 428514\n",
      "c | 118709\n",
      "b | 83080\n",
      "e | 651182\n",
      "d | 242642\n",
      "g | 112092\n",
      "f | 112291\n",
      "i | 357560\n",
      "h | 353582\n",
      "k | 46716\n",
      "j | 8225\n",
      "m | 135069\n",
      "l | 211696\n",
      "o | 402570\n",
      "n | 364930\n",
      "q | 5371\n",
      "p | 85532\n",
      "s | 327557\n",
      "r | 293094\n",
      "u | 147819\n",
      "t | 484089\n",
      "w | 134334\n",
      "v | 46785\n",
      "y | 103313\n",
      "x | 6212\n",
      "{ | 4\n",
      "z | 2515\n",
      "} | 3\n"
     ]
    }
   ],
   "source": [
    "## histogram of the letters\n",
    "l_hist = histogram_letters(uni_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For embedding the sentences, we will use an alphabet with 46 letters with at least 100 occurences in the training set.\n"
     ]
    }
   ],
   "source": [
    "## analysis\n",
    "# 'e', 't' and 'a' are most common letters, which was expectable for a dataset using English language text\n",
    "# I've decided to include punctuation in my model since it could give interesting clues about the novel (or author)\n",
    "# however I've also decided to ignore some rare symbols - less than 100 occurences\n",
    "# the alphabet to consider doesn't include these symbols\n",
    "alphabet = []\n",
    "\n",
    "for letter in l_hist:\n",
    "    if l_hist[letter]>=100:\n",
    "        alphabet = np.append(alphabet, letter)\n",
    "\n",
    "print(\"For embedding the sentences, we will use an alphabet with %i letters with at least 100 occurences in the training set.\"\n",
    "%len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 1, '\"': 2, \"'\": 3, ')': 4, '(': 5, '-': 6, ',': 7, '.': 8, '1': 9, '0': 10, '3': 11, '2': 12, '5': 13, '8': 14, ';': 15, ':': 16, '?': 17, '[': 18, ']': 19, '_': 20, 'a': 21, 'c': 22, 'b': 23, 'e': 24, 'd': 25, 'g': 26, 'f': 27, 'i': 28, 'h': 29, 'k': 30, 'j': 31, 'm': 32, 'l': 33, 'o': 34, 'n': 35, 'q': 36, 'p': 37, 's': 38, 'r': 39, 'u': 40, 't': 41, 'w': 42, 'v': 43, 'y': 44, 'x': 45, 'z': 46}\n"
     ]
    }
   ],
   "source": [
    "enumeration = dict(enumerate(alphabet,1))\n",
    "alphabet_dict = dict (zip(enumeration.values(),enumeration.keys()))\n",
    "\n",
    "print alphabet_dict\n",
    "            \n",
    "X = np.ones((len(uni_x), max_len), dtype=np.int64) *-1\n",
    "\n",
    "for i, sentence in enumerate(uni_x):\n",
    "    for j, char in enumerate(sentence):\n",
    "        if char in alphabet:\n",
    "            X[i,j] = alphabet_dict[char]\n",
    "            \n",
    "# Note: this code leaves rare symbols as -1, which is also used for padding the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence in the test data has 185 symbols, while the shortest has 87 symbols.\n",
      "The average sentence length is 168.027000 and the standard deviation is 8.201480.\n"
     ]
    }
   ],
   "source": [
    "## loading and preparing of test data \n",
    "with open('./data/xtest.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "test_data = text.split('\\n')\n",
    "test_data = filter(None, test_data)\n",
    "test_data = np.asarray(test_data)\n",
    "uni_test = [sen.encode('ascii', 'ignore') for sen in test_data]\n",
    "\n",
    "test_len = [len(sen) for sen in uni_test]\n",
    "min_len = min(test_len)\n",
    "max_len_test = max(test_len)\n",
    "print \"The longest sentence in the test data has %i symbols, while the shortest has %i symbols.\"%(max_len_test,min_len)\n",
    "print(\"The average sentence length is %f and the standard deviation is %f.\"% (np.mean(test_len),np.std(test_len)))\n",
    "\n",
    "# I'm using the same alphabet as extracted from the training data and the maximum length of the training sentences\n",
    "X_test = np.ones((len(uni_test), np.shape(X)[1]), dtype=np.int64) *-1\n",
    "\n",
    "# we cut the sentences of the test set off since we use the maximum length of the training sentences\n",
    "for i, sentence in enumerate(uni_test):\n",
    "    for j, char in enumerate(sentence):\n",
    "        if char in alphabet and j < len(uni_x):\n",
    "            X_test[i,j] = alphabet_dict[char]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## one-hot encoding\n",
    "# we have to embedd our characters so that we can use them as numerical vectors\n",
    "\n",
    "# technically also other embeddings are available such as char2vec but since of the time constraint, I decided to use one-hot\n",
    "# I think a better embedding could lead to a significantly better model\n",
    "\n",
    "# a depth of 6 should be enough since there are 46 characters in this challenge and 2^6 is 64\n",
    "p_depth = 6\n",
    "\n",
    "# defining this function to call it on the fly\n",
    "def transform_one_hot(x, p_depth=6):\n",
    "    return tf.to_float(tf.one_hot(x, p_depth, on_value=1, off_value=0, axis=-1))\n",
    "\n",
    "def one_hot_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], p_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## data split for validation\n",
    "\n",
    "# split into training and validation from train.txt\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, ydata, test_size=0.05, random_state=0)\n",
    "# since our dataset is already pretty small, we can only afford to have a very small validation set\n",
    "\n",
    "# X_test is kept intact for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## resampling\n",
    "# the bias in our data was so big that I decided to resample the training data\n",
    "# this function outputs a binary index vector which samples should be kept for training\n",
    "\n",
    "# sampling ratio can be technically any number but should be greater or equal to 1\n",
    "def sampling_train(y_tr, sampling_ratio=1.0):\n",
    "    bins = range(13)\n",
    "    hist, bin_edges = np.histogram(y_tr, bins, density=False)\n",
    "    # how many samples to take per class - relative to smallest class\n",
    "    n_samples = int(np.min(hist) * sampling_ratio)\n",
    "    \n",
    "    for label in range(len(book_titles)):\n",
    "        idx = np.where(np.equal(y_tr,label))\n",
    "        idx = idx[0]\n",
    "        #shuffle labels\n",
    "        np.random.shuffle(idx)\n",
    "        if n_samples < hist[label]:\n",
    "            idx = idx[:n_samples]\n",
    "        if label == 0:\n",
    "            indices = idx\n",
    "        else:\n",
    "            indices = np.concatenate((indices,idx),axis=0)\n",
    "    np.random.shuffle(indices)\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model design\n",
    "# I'm aware that at this point the standard model for such a task would be a character-level LSTM (RNN)\n",
    "# However, I've been working on CNN for the past 2 years and have more experience with their design. \n",
    "# A quick literature search confirmed that they can be used for this kind of task\n",
    "# [1]\"Convolutional Neural Networks for Sentence Classification\" (2014, https://arxiv.org/pdf/1408.5882.pdf)\n",
    "# [2]\"A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification\" \n",
    "# (2016, https://arxiv.org/pdf/1510.03820.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## (hyper-)parameters\n",
    "# definition of hyperparameters relevant to this task\n",
    "\n",
    "# dropout rate for regularization\n",
    "p_dropout = 0.1\n",
    "# feature map size\n",
    "# adjusted these values by line search as suggested in paper [2]\n",
    "p_filter_nb1 = [200, 200, 200, 200]\n",
    "p_filter_nb2 = [200, 300, 300, 300]\n",
    "p_filter_nb3 = [300, 400, 400, 400]\n",
    "# filter size\n",
    "# adjusted these values by line search as suggested in paper [2]\n",
    "p_filter_len1 = [5, 3, 3, 3]\n",
    "p_filter_len2 = [11, 5, 3, 3]\n",
    "p_filter_len3 = [19, 7, 3, 3]\n",
    "\n",
    "# Load checkpoint if exists\n",
    "checkpoint = False\n",
    "# check if grid search is wanted\n",
    "grid_search = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Defining some necessary functions\n",
    "# 1d max pooling as recommended in [2]\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "# defining a block\n",
    "def conv_block(in_layer, nb_filter=[100, 100, 100], filter_length=[3, 3, 3], subsample=[1, 1, 1], pool_length=[2, 2, 2]):\n",
    "    block = in_layer\n",
    "    for i in range(len(nb_filter)):\n",
    "\n",
    "        block = Conv1D(nb_filter=nb_filter[i],\n",
    "                              filter_length=filter_length[i],\n",
    "                              border_mode='valid',\n",
    "                              activation='relu',\n",
    "                              kernel_initializer='glorot_normal',\n",
    "                              subsample_length=subsample[i])(block)\n",
    "        # TODO: dropout or backnorm (experiment!)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Dropout(p_dropout)(block)\n",
    "        if pool_length[i]:\n",
    "            block = MaxPooling1D(pool_length=pool_length[i])(block)\n",
    "\n",
    "    block = Lambda(max_1d, output_shape=(nb_filter[-1],))(block)\n",
    "    block = Dense(128, activation='relu', kernel_initializer='glorot_normal')(block)\n",
    "    block = Dense(128, activation='relu', kernel_initializer='glorot_normal')(block)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 189)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 189, 6)        0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 185, 200)      6200        lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 179, 200)      13400       lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)                (None, 171, 300)      34500       lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 185, 200)      800         conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 179, 200)      800         conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 171, 300)      1200        conv1d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 185, 200)      0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 179, 200)      0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 171, 300)      0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 92, 200)       0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)   (None, 89, 200)       0           dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)   (None, 85, 300)       0           dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 90, 200)       120200      max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)                (None, 85, 300)       300300      max_pooling1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)               (None, 79, 400)       840400      max_pooling1d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 90, 200)       800         conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 85, 300)       1200        conv1d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 79, 400)       1600        conv1d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 90, 200)       0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 85, 300)       0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 79, 400)       0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 45, 200)       0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)   (None, 42, 300)       0           dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D)  (None, 39, 400)       0           dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 43, 200)       120200      max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 40, 300)       270300      max_pooling1d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)               (None, 37, 400)       480400      max_pooling1d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 43, 200)       800         conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 40, 300)       1200        conv1d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 37, 400)       1600        conv1d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 43, 200)       0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 40, 300)       0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 37, 400)       0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 21, 200)       0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)   (None, 20, 300)       0           dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D)  (None, 18, 400)       0           dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 19, 200)       120200      max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)                (None, 18, 300)       270300      max_pooling1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)               (None, 16, 400)       480400      max_pooling1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 19, 200)       800         conv1d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 18, 300)       1200        conv1d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 16, 400)       1600        conv1d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 19, 200)       0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 18, 300)       0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 16, 400)       0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)   (None, 9, 200)        0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)   (None, 9, 300)        0           dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D)  (None, 8, 400)        0           dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 200)           0           max_pooling1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 300)           0           max_pooling1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, 400)           0           max_pooling1d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           25728       lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           38528       lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 128)           51328       lambda_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           16512       dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 128)           16512       dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 128)           16512       dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 384)           0           dense_2[0][0]                    \n",
      "                                                                   dense_4[0][0]                    \n",
      "                                                                   dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 384)           0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 256)           98560       dropout_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 128)           32896       dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 128)           0           dense_8[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,366,976\n",
      "Trainable params: 3,360,176\n",
      "Non-trainable params: 6,800\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## The main model in keras\n",
    "# insert the input sentence and embedd to one-hot space on-the-fly\n",
    "in_sentence = Input(shape=(max_len,), dtype='int64')\n",
    "embedded = Lambda(transform_one_hot, output_shape=one_hot_outshape)(in_sentence)\n",
    "\n",
    "# convolve with three blocks\n",
    "block1 = conv_block(embedded, p_filter_nb1, filter_length=p_filter_len1, subsample=[1, 1, 1, 1], pool_length=[2, 2, 2, 2])\n",
    "block2 = conv_block(embedded, p_filter_nb2, filter_length=p_filter_len2, subsample=[1, 1, 1, 1], pool_length=[2, 2, 2, 2])\n",
    "block3 = conv_block(embedded, p_filter_nb3, filter_length=p_filter_len3, subsample=[1, 1, 1, 1], pool_length=[2, 2, 2, 2])\n",
    "\n",
    "# merge blocks to one\n",
    "feature_vec = merge([block1,block2, block3], mode='concat', concat_axis=-1)\n",
    "\n",
    "# added fully connected layer\n",
    "feature_vec = Dropout(p_dropout)(feature_vec)\n",
    "feature_vec = Dense(256, activation='relu', kernel_initializer='glorot_normal')(feature_vec)\n",
    "feature_vec = Dense(128, activation='relu', kernel_initializer='glorot_normal')(feature_vec)\n",
    "\n",
    "# send feature vector to encoder\n",
    "to_encode = Dropout(p_dropout)(feature_vec)\n",
    "encoder = Model(input=in_sentence, output=to_encode)\n",
    "    \n",
    "encoder.summary()   \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1, 189)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 128)            3366976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 92)             81328     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 92)                68080     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 12)                1116      \n",
      "=================================================================\n",
      "Total params: 3,517,500\n",
      "Trainable params: 3,510,700\n",
      "Non-trainable params: 6,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## LSTM classifier\n",
    "\n",
    "book_sentences = Input(shape=(1,max_len), dtype='int64')\n",
    "encoded = TimeDistributed(encoder)(book_sentences)\n",
    "\n",
    "lstm_h = 92\n",
    "\n",
    "lstm_layer = LSTM(lstm_h, return_sequences=True, dropout=0.1, recurrent_dropout=0.1, implementation=0)(encoded)\n",
    "lstm_layer2 = LSTM(lstm_h, return_sequences=False, dropout=0.1, recurrent_dropout=0.1, implementation=0)(lstm_layer)\n",
    "\n",
    "# output = Dropout(0.2)(bi_lstm)\n",
    "output = Dense(12, activation='softmax', kernel_initializer='lecun_uniform')(lstm_layer2)\n",
    "\n",
    "model = Model(outputs=output, inputs=book_sentences)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint:\n",
    "    model.load_weights(checkpoint_dir)\n",
    "\n",
    "file_name = 'char-level-cnn'\n",
    "check_cb = keras.callbacks.ModelCheckpoint('checkpoints/' + file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                            monitor='val_loss',\n",
    "                                            verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "# TODO: set learning rate decay\n",
    "optimizer = RMSprop(lr=0.001, decay=0.0001)\n",
    "# using loss for multiclass problem\n",
    "# combination of categorical_crossentropy and softmax good for multiclass label problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# following the keras documentation, I've realized that the labels have to be formatted this way in order to use\n",
    "# the categorical cross entropy loss\n",
    "\n",
    "y_val = keras.utils.to_categorical(y_val, 12)\n",
    "X_val = X_val.reshape(len(X_val), 1, max_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17002 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "17002/17002 [==============================] - 44s - loss: 2.8289 - acc: 0.1685 - val_loss: 2.6635 - val_acc: 0.0159\n",
      "Epoch 2/5\n",
      "17002/17002 [==============================] - 42s - loss: 2.8170 - acc: 0.1700 - val_loss: 2.5615 - val_acc: 0.1100\n",
      "Epoch 3/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8113 - acc: 0.1713 - val_loss: 2.8370 - val_acc: 0.1100\n",
      "Epoch 4/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8089 - acc: 0.1702 - val_loss: 2.6818 - val_acc: 0.0593\n",
      "Epoch 5/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8190 - acc: 0.1659 - val_loss: 3.3885 - val_acc: 0.1100\n",
      "Train on 17002 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8151 - acc: 0.1705 - val_loss: 3.5514 - val_acc: 0.1100\n",
      "Epoch 2/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8252 - acc: 0.1696 - val_loss: 2.5556 - val_acc: 0.1100\n",
      "Epoch 3/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8104 - acc: 0.1651 - val_loss: 2.8334 - val_acc: 0.1100\n",
      "Epoch 4/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8169 - acc: 0.1665 - val_loss: 3.0983 - val_acc: 0.1100\n",
      "Epoch 5/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8158 - acc: 0.1673 - val_loss: 2.9354 - val_acc: 0.1100\n",
      "Train on 17002 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8015 - acc: 0.1695 - val_loss: 5.7885 - val_acc: 0.1100\n",
      "Epoch 2/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8097 - acc: 0.1676 - val_loss: 5.7572 - val_acc: 0.1100\n",
      "Epoch 3/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8094 - acc: 0.1720 - val_loss: 5.1485 - val_acc: 0.1100\n",
      "Epoch 4/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8000 - acc: 0.1702 - val_loss: 6.0871 - val_acc: 0.1100\n",
      "Epoch 5/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8070 - acc: 0.1695 - val_loss: 5.7570 - val_acc: 0.1100\n",
      "Train on 17002 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.7958 - acc: 0.1677 - val_loss: 5.0141 - val_acc: 0.1100\n",
      "Epoch 2/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8013 - acc: 0.1713 - val_loss: 5.1169 - val_acc: 0.1100\n",
      "Epoch 3/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8006 - acc: 0.1741 - val_loss: 5.3934 - val_acc: 0.1100\n",
      "Epoch 4/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8043 - acc: 0.1711 - val_loss: 3.8404 - val_acc: 0.1100\n",
      "Epoch 5/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8133 - acc: 0.1690 - val_loss: 4.3053 - val_acc: 0.1100\n",
      "Train on 17002 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8076 - acc: 0.1712 - val_loss: 5.5965 - val_acc: 0.1100\n",
      "Epoch 2/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8012 - acc: 0.1717 - val_loss: 6.5552 - val_acc: 0.1100\n",
      "Epoch 3/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8028 - acc: 0.1717 - val_loss: 6.6113 - val_acc: 0.1100\n",
      "Epoch 4/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8030 - acc: 0.1717 - val_loss: 6.4577 - val_acc: 0.1100\n",
      "Epoch 5/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.7992 - acc: 0.1673 - val_loss: 5.7821 - val_acc: 0.1100\n",
      "Train on 17002 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.7948 - acc: 0.1734 - val_loss: 5.4943 - val_acc: 0.1100\n",
      "Epoch 2/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.7950 - acc: 0.1705 - val_loss: 5.6329 - val_acc: 0.1100\n",
      "Epoch 3/5\n",
      "17002/17002 [==============================] - 43s - loss: 2.8048 - acc: 0.1735 - val_loss: 5.7753 - val_acc: 0.1100\n",
      "Epoch 4/5\n",
      "17002/17002 [==============================] - 83s - loss: 2.7987 - acc: 0.1730 - val_loss: 6.2567 - val_acc: 0.1100\n",
      "Epoch 5/5\n",
      "17002/17002 [==============================] - 105s - loss: 2.7935 - acc: 0.1733 - val_loss: 6.3732 - val_acc: 0.1100\n",
      "Train on 17002 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "17002/17002 [==============================] - 105s - loss: 2.7994 - acc: 0.1677 - val_loss: 6.7034 - val_acc: 0.1100\n",
      "Epoch 2/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.8083 - acc: 0.1699 - val_loss: 5.6614 - val_acc: 0.1100\n",
      "Epoch 3/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.8005 - acc: 0.1740 - val_loss: 5.6538 - val_acc: 0.1100\n",
      "Epoch 4/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.7999 - acc: 0.1716 - val_loss: 5.5506 - val_acc: 0.1100\n",
      "Epoch 5/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.7969 - acc: 0.1747 - val_loss: 5.9606 - val_acc: 0.1100\n",
      "Train on 17002 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.8096 - acc: 0.1729 - val_loss: 5.5471 - val_acc: 0.1100\n",
      "Epoch 2/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.8081 - acc: 0.1723 - val_loss: 5.4271 - val_acc: 0.1100\n",
      "Epoch 3/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.8050 - acc: 0.1725 - val_loss: 5.8646 - val_acc: 0.1100\n",
      "Epoch 4/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.7985 - acc: 0.1698 - val_loss: 6.2795 - val_acc: 0.1100\n",
      "Epoch 5/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.8039 - acc: 0.1682 - val_loss: 5.0413 - val_acc: 0.1100\n",
      "Train on 17002 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.8031 - acc: 0.1707 - val_loss: 5.5753 - val_acc: 0.1100\n",
      "Epoch 2/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.7964 - acc: 0.1758 - val_loss: 5.7872 - val_acc: 0.1100\n",
      "Epoch 3/5\n",
      "17002/17002 [==============================] - 104s - loss: 2.7961 - acc: 0.1754 - val_loss: 6.0077 - val_acc: 0.1100\n",
      "Epoch 4/5\n",
      "17002/17002 [==============================] - 105s - loss: 2.7926 - acc: 0.1709 - val_loss: 5.7794 - val_acc: 0.1100\n",
      "Epoch 5/5\n",
      "10400/17002 [=================>............] - ETA: 39s - loss: 2.7838 - acc: 0.1732"
     ]
    }
   ],
   "source": [
    "# the actual training is done here\n",
    "keys = range(0,len(book_titles))\n",
    "bins = range(13)\n",
    "    \n",
    "if grid_search:\n",
    "    mu = np.linspace(0.001,1.0,20)\n",
    "    y_train = keras.utils.to_categorical(y_train, 12)\n",
    "    X_train = X_train.reshape(len(X_train), 1, max_len)   \n",
    "    # random labels_dict\n",
    "    hist, bin_edges = np.histogram(ydata, bins, density=False)\n",
    "    \n",
    "    labels_dict = dict((keys[i],hist[i]) for i in range(len(book_titles)))\n",
    "    # grid search for good mu\n",
    "    for i in mu:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        class_weights = create_class_weight(labels_dict,i)\n",
    "        # the actual training is done here\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val), class_weight=class_weights, batch_size=512, epochs=10, verbose=1, shuffle=True, callbacks=[check_cb, earlystop_cb])\n",
    "        score = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(\"Test loss: %f,  Test accuracy: %f   ,mu  %f\" % (score[0],score[1],i))\n",
    "else:\n",
    "    mu = 0.3\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    #class_weights = create_class_weight(labels_dict,mu)\n",
    "    for i in range(15):\n",
    "        # with this split, it's using about half the training samples\n",
    "        indices = sampling_train(y_train,3.0)\n",
    "        \n",
    "        X_sampled = X_train[indices,:]\n",
    "        y_sampled = [y_train[j] for j in indices]\n",
    "        \n",
    "        hist, bin_edges = np.histogram(y_sampled, bins, density=False)\n",
    "        \n",
    "        y_sampled = keras.utils.to_categorical(y_sampled, 12)\n",
    "        X_sampled = X_sampled.reshape(len(X_sampled), 1, max_len)    \n",
    "       \n",
    "        labels_dict = dict((keys[i],hist[i]) for i in range(len(book_titles)))\n",
    "        class_weights = create_class_weight(labels_dict,mu)\n",
    "        \n",
    "        \n",
    "        model.fit(X_sampled, y_sampled, validation_data=(X_val, y_val), class_weight=class_weights, batch_size=32, epochs=5, shuffle=True, callbacks=[check_cb, earlystop_cb])\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(\"Test loss: %f,  Test accuracy: %f   ,mu  %f\" % (score[0],score[1],mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing\n",
    "# use model to predict labels for test sentences\n",
    "X_test = X_test.reshape(len(X_test), 1, max_len) \n",
    "y_pred = model.predict(X_test)\n",
    "# convert softmax probabilities to class labels\n",
    "y_label = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmZWmpOImZmV5iRiZmal/X8J48DfYD7LcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f62302d1450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity test for labels\n",
    "hist, bin_edges = np.histogram(y_label, bins, density=False)\n",
    "# shows a nice plot of the histogram of labels\n",
    "plt.title(\"Predicted Label Histogram\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency in percentage\")\n",
    "plt.bar(bin_edges[:-1], hist, width=1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()  \n",
    "\n",
    "# we see that the training did not work, the model can not discriminate between the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write test sentences to txt file\n",
    "np.savetxt(\"ytest.txt\", y_label, fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
